{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Урок 2. Создание признакового пространства"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. Создайте мешок слов с помощью sklearn.feature_extraction.text.CountVectorizer.fit_transform(). Применим его к 'tweet_stemmed' и 'tweet_lemmatized' отдельно.\n",
    " - Игнорируем слова, частота которых в документе строго превышает порог 0.9 с помощью max_df.\n",
    " - Ограничим количество слов, попадающий в мешок, с помощью max_features = 1000.\n",
    " - Исключим стоп-слова с помощью stop_words='english'. \n",
    " - Отобразим Bag-of-Words модель как DataFrame. columns необходимо извлечь с помощью CountVectorizer.get_feature_names().\n",
    " \n",
    "2. Создайте мешок слов с помощью sklearn.feature_extraction.text.TfidfVectorizer.fit_transform(). Применим его к 'tweet_stemmed' и 'tweet_lemmatized' отдельно.\n",
    " - Игнорируем слова, частота которых в документе строго превышает порог 0.9 с помощью max_df.\n",
    " - Ограничим количество слов, попадающий в мешок, с помощью max_features = 1000.\n",
    " - Исключим стоп-слова с помощью stop_words='english'.\n",
    " - Отобразим Bag-of-Words модель как DataFrame. columns необходимо извлечь с помощью TfidfVectorizer.get_feature_names().\n",
    " \n",
    "3. Проверьте ваши векторайзеры на корпусе который использовали на вебинаре, составьте таблицу метод векторизации и скор который вы получили (в методах векторизации по изменяйте параметры что бы добиться лучшего скора) обратите внимание как падает/растёт скор при уменьшении количества фичей, и изменении параметров, так же попробуйте применить к векторайзерам PCA для сокращения размерности посмотрите на качество сделайте выводы."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Дополнительные задания из старого файла.  \n",
    "\n",
    "4. Натренируем gensim.models.Word2Vec модель на наших данных.\n",
    "Тренировать будем на токенизированных твитах combine_df['tweet_token']\n",
    "Установим следующие параметры: size=200, window=5, min_count=2, sg = 1, hs = 0, negative = 10, workers= 32, seed = 34.\n",
    "Используем функцию train() с параметром total_examples равным длине combine_df['tweet_token'], количество epochs установим 20.\n",
    "\n",
    "\n",
    "5. Давайте немного потестируем нашу модель Word2Vec и посмотрим, как она работает. Мы зададим слово positive = \"dinner\", и модель вытащит из корпуса наиболее похожие слова c помощью функции most_similar. То же самое попробуем со словом \"trump\".\n",
    "\n",
    "\n",
    "6. Из приведенных выше примеров мы видим, что наша модель word2vec хорошо справляется с поиском наиболее похожих слов для данного слова. Но как она это делает? Она изучила векторы для каждого уникального слова наших данных и использует косинусное сходство, чтобы найти наиболее похожие векторы (слова).\n",
    "Давайте проверим векторное представление любого слова из нашего корпуса, например \"food\".\n",
    "\n",
    "\n",
    "7. Поскольку наши данные содержат твиты, а не только слова, нам придется придумать способ использовать векторы слов из модели word2vec для создания векторного представления всего твита. Существует простое решение этой проблемы, мы можем просто взять среднее значение всех векторов слов, присутствующих в твите. Длина результирующего вектора будет одинаковой, то есть 200. Мы повторим тот же процесс для всех твитов в наших данных и получим их векторы. Теперь у нас есть 200 функций word2vec для наших данных.\n",
    "Необходимо создать вектор для каждого твита, взяв среднее значение векторов слов, присутствующих в твите. В цикле сделать:  vec += model_w2v[word].reshape((1, size)) и поделить финальный вектор на количество слов в твите.\n",
    "\n",
    "На выходе должен получиться wordvec_df.shape = (49159, 200)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Решение."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Обработанные данные открыть из файла который мы сохранили в прошом ДЗ."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "sklearn.feature_extraction.text.CountVectorizer.fit_transform()\n",
    "\n",
    "1. Создайте мешок слов с помощью sklearn.feature_extraction.text.CountVectorizer.fit_transform(). Применим его к 'tweet_stemmed' и 'tweet_lemmatized' отдельно.\n",
    " - Игнорируем слова, частота которых в документе строго превышает порог 0.9 с помощью max_df.\n",
    " - Ограничим количество слов, попадающий в мешок, с помощью max_features = 1000.\n",
    " - Исключим стоп-слова с помощью stop_words='english'.\n",
    " - Отобразим Bag-of-Words модель как DataFrame. columns необходимо извлечь с помощью CountVectorizer.get_feature_names()."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "with open('combine_df.pickle', 'rb') as handle:\n",
    "    df = pickle.load(handle)\n",
    "df.head(3)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   id  label                                              tweet  \\\n",
       "0   1    0.0  when father is dysfunctional and is so selfish...   \n",
       "1   2    0.0  thanks for lyft credit cannot use cause they d...   \n",
       "2   3    0.0                                bihday your majesty   \n",
       "\n",
       "                                         tweet_token  \\\n",
       "0  [when, father, is, dysfunctional, and, is, so,...   \n",
       "1  [thanks, for, lyft, credit, can, not, use, cau...   \n",
       "2                            [bihday, your, majesty]   \n",
       "\n",
       "                                tweet_token_filtered  \\\n",
       "0  [father, dysfunctional, selfish, drags, kids, ...   \n",
       "1  [thanks, lyft, credit, use, cause, offer, whee...   \n",
       "2                                  [bihday, majesty]   \n",
       "\n",
       "                                       tweet_stemmed  \\\n",
       "0  [father, dysfunct, selfish, drag, kid, dysfunc...   \n",
       "1  [thank, lyft, credit, use, caus, offer, wheelc...   \n",
       "2                                  [bihday, majesti]   \n",
       "\n",
       "                                    tweet_lemmatized  \n",
       "0  [father, dysfunctional, selfish, drag, kid, dy...  \n",
       "1  [thanks, lyft, credit, use, cause, offer, whee...  \n",
       "2                                  [bihday, majesty]  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_token</th>\n",
       "      <th>tweet_token_filtered</th>\n",
       "      <th>tweet_stemmed</th>\n",
       "      <th>tweet_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>when father is dysfunctional and is so selfish...</td>\n",
       "      <td>[when, father, is, dysfunctional, and, is, so,...</td>\n",
       "      <td>[father, dysfunctional, selfish, drags, kids, ...</td>\n",
       "      <td>[father, dysfunct, selfish, drag, kid, dysfunc...</td>\n",
       "      <td>[father, dysfunctional, selfish, drag, kid, dy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>thanks for lyft credit cannot use cause they d...</td>\n",
       "      <td>[thanks, for, lyft, credit, can, not, use, cau...</td>\n",
       "      <td>[thanks, lyft, credit, use, cause, offer, whee...</td>\n",
       "      <td>[thank, lyft, credit, use, caus, offer, wheelc...</td>\n",
       "      <td>[thanks, lyft, credit, use, cause, offer, whee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>[bihday, your, majesty]</td>\n",
       "      <td>[bihday, majesty]</td>\n",
       "      <td>[bihday, majesti]</td>\n",
       "      <td>[bihday, majesty]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Соберем все уникальные слова из 4 загруженных предложений, игнорируя регистр, пунктуацию и односимвольные токены. Это и будет наш словарь (известные слова)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "df['tweet_stemmed'] = df['tweet_stemmed'].apply(lambda x:' '.join(x))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "df['tweet_lemmatized'] = df['tweet_lemmatized'].apply(lambda x:' '.join(x))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "count_vectorizer = CountVectorizer(ngram_range=(1, 1),\n",
    "                                   analyzer='word',\n",
    "                                   binary=False,\n",
    "                                   max_df=0.9,\n",
    "                                   stop_words='english',\n",
    "                                   max_features=1000)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Создаем the Bag-of-Words модель\n",
    "tweet_stemmed_bow = count_vectorizer.fit_transform(df['tweet_stemmed'].values)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Отобразим Bag-of-Words модель как DataFrame\n",
    "feature_names = count_vectorizer.get_feature_names()\n",
    "tweet_stemmed_df = pd.DataFrame(tweet_stemmed_bow.toarray(), columns = feature_names)\n",
    "\n",
    "tweet_stemmed_df.head(3)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   abl  absolut  accept  account  act  action  actor  actual  ad  adapt  ...  \\\n",
       "0    0        0       0        0    0       0      0       0   0      0  ...   \n",
       "1    0        0       0        0    0       0      0       0   0      0  ...   \n",
       "2    0        0       0        0    0       0      0       0   0      0  ...   \n",
       "\n",
       "   yeah  year  yesterday  yo  yoga  york  young  youtub  yr  yummi  \n",
       "0     0     0          0   0     0     0      0       0   0      0  \n",
       "1     0     0          0   0     0     0      0       0   0      0  \n",
       "2     0     0          0   0     0     0      0       0   0      0  \n",
       "\n",
       "[3 rows x 1000 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abl</th>\n",
       "      <th>absolut</th>\n",
       "      <th>accept</th>\n",
       "      <th>account</th>\n",
       "      <th>act</th>\n",
       "      <th>action</th>\n",
       "      <th>actor</th>\n",
       "      <th>actual</th>\n",
       "      <th>ad</th>\n",
       "      <th>adapt</th>\n",
       "      <th>...</th>\n",
       "      <th>yeah</th>\n",
       "      <th>year</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yo</th>\n",
       "      <th>yoga</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>youtub</th>\n",
       "      <th>yr</th>\n",
       "      <th>yummi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 1000 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# Создаем the Bag-of-Words модель\n",
    "tweet_lemmatized_bow = count_vectorizer.fit_transform(df['tweet_lemmatized'].values)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# Отобразим Bag-of-Words модель как DataFrame\n",
    "feature_names = count_vectorizer.get_feature_names()\n",
    "tweet_lemmatized_df = pd.DataFrame(tweet_lemmatized_bow.toarray(), columns = feature_names)\n",
    "\n",
    "tweet_lemmatized_df.head(3)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   able  absolutely  account  act  action  actor  actually  adapt  add  \\\n",
       "0     0           0        0    0       0      0         0      0    0   \n",
       "1     0           0        0    0       0      0         0      0    0   \n",
       "2     0           0        0    0       0      0         0      0    0   \n",
       "\n",
       "   adventure  ...  year  yes  yesterday  yo  yoga  york  young  youtube  yr  \\\n",
       "0          0  ...     0    0          0   0     0     0      0        0   0   \n",
       "1          0  ...     0    0          0   0     0     0      0        0   0   \n",
       "2          0  ...     0    0          0   0     0     0      0        0   0   \n",
       "\n",
       "   yummy  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "\n",
       "[3 rows x 1000 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>able</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>account</th>\n",
       "      <th>act</th>\n",
       "      <th>action</th>\n",
       "      <th>actor</th>\n",
       "      <th>actually</th>\n",
       "      <th>adapt</th>\n",
       "      <th>add</th>\n",
       "      <th>adventure</th>\n",
       "      <th>...</th>\n",
       "      <th>year</th>\n",
       "      <th>yes</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yo</th>\n",
       "      <th>yoga</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>youtube</th>\n",
       "      <th>yr</th>\n",
       "      <th>yummy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 1000 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. Создайте мешок слов с помощью sklearn.feature_extraction.text.TfidfVectorizer.fit_transform(). Применим его к 'tweet_stemmed' и 'tweet_lemmatized' отдельно.\n",
    " - Игнорируем слова, частота которых в документе строго превышает порог 0.9 с помощью max_df.\n",
    " - Ограничим количество слов, попадающий в мешок, с помощью max_features = 1000.\n",
    " - Исключим стоп-слова с помощью stop_words='english'.\n",
    " - Отобразим Bag-of-Words модель как DataFrame. columns необходимо извлечь с помощью TfidfVectorizer.get_feature_names()."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 1),\n",
    "                                   analyzer='word',\n",
    "                                   binary=False,\n",
    "                                   max_df=0.9,\n",
    "                                   stop_words='english',\n",
    "                                   max_features=1000)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# Создаем the Bag-of-Words модель\n",
    "tweet_stemmed_bow_tfidf = count_vectorizer.fit_transform(df['tweet_stemmed'].values)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# Отобразим Bag-of-Words модель как DataFrame\n",
    "feature_names = count_vectorizer.get_feature_names()\n",
    "tweet_stemmed_tfidf_df = pd.DataFrame(tweet_stemmed_bow_tfidf.toarray(), columns = feature_names)\n",
    "\n",
    "tweet_stemmed_tfidf_df.head(3)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   abl  absolut  accept  account  act  action  actor  actual  ad  adapt  ...  \\\n",
       "0    0        0       0        0    0       0      0       0   0      0  ...   \n",
       "1    0        0       0        0    0       0      0       0   0      0  ...   \n",
       "2    0        0       0        0    0       0      0       0   0      0  ...   \n",
       "\n",
       "   yeah  year  yesterday  yo  yoga  york  young  youtub  yr  yummi  \n",
       "0     0     0          0   0     0     0      0       0   0      0  \n",
       "1     0     0          0   0     0     0      0       0   0      0  \n",
       "2     0     0          0   0     0     0      0       0   0      0  \n",
       "\n",
       "[3 rows x 1000 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abl</th>\n",
       "      <th>absolut</th>\n",
       "      <th>accept</th>\n",
       "      <th>account</th>\n",
       "      <th>act</th>\n",
       "      <th>action</th>\n",
       "      <th>actor</th>\n",
       "      <th>actual</th>\n",
       "      <th>ad</th>\n",
       "      <th>adapt</th>\n",
       "      <th>...</th>\n",
       "      <th>yeah</th>\n",
       "      <th>year</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yo</th>\n",
       "      <th>yoga</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>youtub</th>\n",
       "      <th>yr</th>\n",
       "      <th>yummi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 1000 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# Создаем the Bag-of-Words модель\n",
    "tweet_lemmatized_bow_tfidf = count_vectorizer.fit_transform(df['tweet_lemmatized'].values)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# Отобразим Bag-of-Words модель как DataFrame\n",
    "feature_names = count_vectorizer.get_feature_names()\n",
    "tweet_lemmatized_tfidf_df = pd.DataFrame(tweet_lemmatized_bow_tfidf.toarray(), columns = feature_names)\n",
    "\n",
    "tweet_lemmatized_tfidf_df.head(3)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   able  absolutely  account  act  action  actor  actually  adapt  add  \\\n",
       "0     0           0        0    0       0      0         0      0    0   \n",
       "1     0           0        0    0       0      0         0      0    0   \n",
       "2     0           0        0    0       0      0         0      0    0   \n",
       "\n",
       "   adventure  ...  year  yes  yesterday  yo  yoga  york  young  youtube  yr  \\\n",
       "0          0  ...     0    0          0   0     0     0      0        0   0   \n",
       "1          0  ...     0    0          0   0     0     0      0        0   0   \n",
       "2          0  ...     0    0          0   0     0     0      0        0   0   \n",
       "\n",
       "   yummy  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "\n",
       "[3 rows x 1000 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>able</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>account</th>\n",
       "      <th>act</th>\n",
       "      <th>action</th>\n",
       "      <th>actor</th>\n",
       "      <th>actually</th>\n",
       "      <th>adapt</th>\n",
       "      <th>add</th>\n",
       "      <th>adventure</th>\n",
       "      <th>...</th>\n",
       "      <th>year</th>\n",
       "      <th>yes</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yo</th>\n",
       "      <th>yoga</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>youtube</th>\n",
       "      <th>yr</th>\n",
       "      <th>yummy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 1000 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. Проверьте ваши векторайзеры на корпусе который использовали на вебинаре.<br> \n",
    "Составьте таблицу метод векторизации и скор который вы получили (в методах векторизации по изменяйте параметры что бы добиться лучшего скора).<br>\n",
    " Обратите внимание как падает/растёт скор при уменьшении количества фичей, и изменении параметров, <br>\n",
    " Так же попробуйте применить к векторайзерам **PCA** для сокращения размерности посмотрите на качество сделайте выводы."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "tweet_stemmed_df\n",
    "tweet_lemmatized_df\n",
    "\n",
    "tweet_stemmed_bow_tfidf\n",
    "tweet_lemmatized_tfidf_df"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       able  absolutely  account  act  action  actor  actually  adapt  add  \\\n",
       "0         0           0        0    0       0      0         0      0    0   \n",
       "1         0           0        0    0       0      0         0      0    0   \n",
       "2         0           0        0    0       0      0         0      0    0   \n",
       "3         0           0        0    0       0      0         0      0    0   \n",
       "4         0           0        0    0       0      0         0      0    0   \n",
       "...     ...         ...      ...  ...     ...    ...       ...    ...  ...   \n",
       "49154     0           0        0    0       0      0         0      0    0   \n",
       "49155     0           0        0    0       0      0         0      0    0   \n",
       "49156     0           0        0    0       0      0         0      0    0   \n",
       "49157     0           0        0    0       0      0         0      0    0   \n",
       "49158     0           0        0    0       0      0         0      0    0   \n",
       "\n",
       "       adventure  ...  year  yes  yesterday  yo  yoga  york  young  youtube  \\\n",
       "0              0  ...     0    0          0   0     0     0      0        0   \n",
       "1              0  ...     0    0          0   0     0     0      0        0   \n",
       "2              0  ...     0    0          0   0     0     0      0        0   \n",
       "3              0  ...     0    0          0   0     0     0      0        0   \n",
       "4              0  ...     0    0          0   0     0     0      0        0   \n",
       "...          ...  ...   ...  ...        ...  ..   ...   ...    ...      ...   \n",
       "49154          0  ...     0    0          0   0     0     0      0        0   \n",
       "49155          0  ...     0    0          0   0     0     0      0        0   \n",
       "49156          0  ...     0    0          0   0     0     0      0        0   \n",
       "49157          0  ...     0    0          0   0     0     0      0        0   \n",
       "49158          0  ...     0    0          0   0     0     0      0        0   \n",
       "\n",
       "       yr  yummy  \n",
       "0       0      0  \n",
       "1       0      0  \n",
       "2       0      0  \n",
       "3       0      0  \n",
       "4       0      0  \n",
       "...    ..    ...  \n",
       "49154   0      0  \n",
       "49155   0      0  \n",
       "49156   0      0  \n",
       "49157   0      0  \n",
       "49158   0      0  \n",
       "\n",
       "[49159 rows x 1000 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>able</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>account</th>\n",
       "      <th>act</th>\n",
       "      <th>action</th>\n",
       "      <th>actor</th>\n",
       "      <th>actually</th>\n",
       "      <th>adapt</th>\n",
       "      <th>add</th>\n",
       "      <th>adventure</th>\n",
       "      <th>...</th>\n",
       "      <th>year</th>\n",
       "      <th>yes</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yo</th>\n",
       "      <th>yoga</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>youtube</th>\n",
       "      <th>yr</th>\n",
       "      <th>yummy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49154</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49155</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49156</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49157</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49158</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49159 rows × 1000 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# Загружаем данные\n",
    "data = open('corpus').read()\n",
    "labels, texts = [], []\n",
    "for i, line in enumerate(data.split(\"\\n\")):\n",
    "    content = line.split()\n",
    "    labels.append(content[0])\n",
    "    texts.append(\" \".join(content[1:]))\n",
    "\n",
    "# создаем df\n",
    "trainDF = pd.DataFrame()\n",
    "trainDF['text'] = texts\n",
    "trainDF['label'] = labels\n",
    "trainDF.head(5)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                text       label\n",
       "0  Stuning even for the non-gamer: This sound tra...  __label__2\n",
       "1  The best soundtrack ever to anything.: I'm rea...  __label__2\n",
       "2  Amazing!: This soundtrack is my favorite music...  __label__2\n",
       "3  Excellent Soundtrack: I truly like this soundt...  __label__2\n",
       "4  Remember, Pull Your Jaw Off The Floor After He...  __label__2"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Stuning even for the non-gamer: This sound tra...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The best soundtrack ever to anything.: I'm rea...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amazing!: This soundtrack is my favorite music...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Excellent Soundtrack: I truly like this soundt...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Remember, Pull Your Jaw Off The Floor After He...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "# Создадим словарь, в котрый будем записывать результаты рабты. \n",
    "\n",
    "vectorizes_table_compare = {'vectorizer':[],\n",
    "'features':[], \n",
    "'score': [],\n",
    "'pca_components':[],\n",
    "'pca_score': []\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "vectorizes_table_compare.keys()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dict_keys(['vectorizer', 'features', 'score', 'pca_components', 'pca_score'])"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['text'], trainDF['label'])\n",
    "\n",
    "# labelEncode целевую переменную\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)\n",
    "\n",
    "n_teatures_list = [100, 200,300,500,800,1000,None]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## CountVectorizer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "for n_features in n_teatures_list:\n",
    "    count_vect = CountVectorizer(analyzer='word',\n",
    "                                token_pattern=r'\\w{1,}',\n",
    "                                max_df=0.9,\n",
    "                                stop_words='english',\n",
    "                                max_features=n_features)\n",
    "\n",
    "    count_vect.fit(trainDF['text'])\n",
    "\n",
    "    xtrain_count =  count_vect.transform(train_x)\n",
    "    xvalid_count =  count_vect.transform(valid_x)\n",
    "\n",
    "    classifier = linear_model.LogisticRegression()\n",
    "    classifier.fit(xtrain_count, train_y)\n",
    "    predictions = classifier.predict(xvalid_count)\n",
    "\n",
    "    score = accuracy_score(valid_y, predictions)\n",
    "    # print(score)\n",
    "    vectorizes_table_compare['score'].append(score)\n",
    "    vectorizes_table_compare['vectorizer'].append('CountVectorizer')\n",
    "    vectorizes_table_compare['features'].append(n_features)\n",
    " #=============================================================\n",
    "    if n_features == 100:\n",
    "       n_components=100\n",
    "    else:\n",
    "       n_components=200 \n",
    "\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_train = pca.fit_transform(xtrain_count.toarray())\n",
    "    X_test = pca.transform(xvalid_count.toarray())\n",
    "\n",
    "    classifier = linear_model.LogisticRegression()\n",
    "    classifier.fit(X_train, train_y)\n",
    "    predictions = classifier.predict(X_test)\n",
    "\n",
    "    score = accuracy_score(valid_y, predictions)\n",
    "    vectorizes_table_compare['pca_components'].append(n_components)\n",
    "    vectorizes_table_compare['pca_score'].append(score)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## HashingVectorizer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for n_features in n_teatures_list:\n",
    "    \n",
    "    if n_features is None:\n",
    "        n_features= 10000   # 1048576\n",
    "    vct = HashingVectorizer(analyzer='word', \n",
    "                            token_pattern=r'\\w{1,}', \n",
    "                            n_features=n_features) \n",
    " \n",
    "    vct.fit(trainDF['text'])\n",
    "    xtrain_hash =  vct.transform(train_x)\n",
    "    xvalid_hash =  vct.transform(valid_x)\n",
    "\n",
    "    classifier = linear_model.LogisticRegression()\n",
    "    classifier.fit(xtrain_hash, train_y)\n",
    "    predictions = classifier.predict(xvalid_hash)\n",
    "\n",
    "    score = accuracy_score(valid_y, predictions)\n",
    "\n",
    "    vectorizes_table_compare['score'].append(score)\n",
    "    vectorizes_table_compare['vectorizer'].append('HashingVectorizer')\n",
    "    vectorizes_table_compare['features'].append(n_features)\n",
    " #=============================================================\n",
    "    if n_features == 100:\n",
    "       n_components=100\n",
    "   #  elif n_components is None:\n",
    "   #     continue\n",
    "    else:\n",
    "       n_components=200 \n",
    "\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_train = pca.fit_transform(xtrain_hash.toarray())\n",
    "    X_test = pca.transform(xvalid_hash.toarray())\n",
    "\n",
    "    classifier = linear_model.LogisticRegression()\n",
    "    classifier.fit(X_train, train_y)\n",
    "    predictions = classifier.predict(X_test)\n",
    "\n",
    "    score = accuracy_score(valid_y, predictions)\n",
    "    vectorizes_table_compare['pca_components'].append(n_components)\n",
    "    vectorizes_table_compare['pca_score'].append(score)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TfidfVectorizer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "for n_features in n_teatures_list:\n",
    "    tfidf_vectorizer = TfidfVectorizer(\n",
    "                                    analyzer='word',\n",
    "                                    token_pattern=r'\\w{1,}',\n",
    "                                    binary=False,\n",
    "                                    max_df=0.9,\n",
    "                                    stop_words='english',\n",
    "                                    max_features=n_features,\n",
    "                                    )\n",
    "\n",
    "    tfidf_vectorizer.fit(trainDF['text'])\n",
    "\n",
    "    xtrain_tfidf =  tfidf_vectorizer.transform(train_x)\n",
    "    xvalid_tfidf =  tfidf_vectorizer.transform(valid_x)\n",
    "\n",
    "    classifier = linear_model.LogisticRegression()\n",
    "    classifier.fit(xtrain_tfidf, train_y)\n",
    "    predictions = classifier.predict(xvalid_tfidf)\n",
    "    # predictions\n",
    "    score = accuracy_score(valid_y, predictions)\n",
    "    \n",
    "    vectorizes_table_compare['vectorizer'].append('TfidfVectorizer')\n",
    "    vectorizes_table_compare['features'].append(n_features)\n",
    "    vectorizes_table_compare['score'].append(score)\n",
    "    \n",
    "#=============================================================\n",
    "    if n_features == 100:\n",
    "       n_components=100\n",
    "    else:\n",
    "       n_components=200 \n",
    "\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_train = pca.fit_transform(xtrain_tfidf.toarray())\n",
    "    X_test = pca.transform(xvalid_tfidf.toarray())\n",
    "\n",
    "    classifier = linear_model.LogisticRegression()\n",
    "    classifier.fit(X_train, train_y)\n",
    "    predictions = classifier.predict(X_test)\n",
    "\n",
    "    score = accuracy_score(valid_y, predictions)\n",
    "    vectorizes_table_compare['pca_components'].append(n_components)\n",
    "    vectorizes_table_compare['pca_score'].append(score)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "result_df = pd.DataFrame(vectorizes_table_compare)\n",
    "result_df"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "           vectorizer  features   score  pca_components  pca_score\n",
       "0     CountVectorizer     100.0  0.7480             100     0.7480\n",
       "1     CountVectorizer     200.0  0.7948             200     0.7948\n",
       "2     CountVectorizer     300.0  0.8000             200     0.7964\n",
       "3     CountVectorizer     500.0  0.8144             200     0.8036\n",
       "4     CountVectorizer     800.0  0.8184             200     0.8128\n",
       "5     CountVectorizer    1000.0  0.8156             200     0.8084\n",
       "6     CountVectorizer       NaN  0.8484             200     0.8184\n",
       "7   HashingVectorizer     100.0  0.6652             100     0.6652\n",
       "8   HashingVectorizer     200.0  0.7112             200     0.7116\n",
       "9   HashingVectorizer     300.0  0.7252             200     0.7204\n",
       "10  HashingVectorizer     500.0  0.7772             200     0.7632\n",
       "11  HashingVectorizer     800.0  0.7836             200     0.7752\n",
       "12  HashingVectorizer    1000.0  0.7924             200     0.7748\n",
       "13  HashingVectorizer   10000.0  0.8212             200     0.7956\n",
       "14    TfidfVectorizer     100.0  0.7504             100     0.7504\n",
       "15    TfidfVectorizer     200.0  0.7984             200     0.7984\n",
       "16    TfidfVectorizer     300.0  0.8020             200     0.8008\n",
       "17    TfidfVectorizer     500.0  0.8168             200     0.8088\n",
       "18    TfidfVectorizer     800.0  0.8228             200     0.8208\n",
       "19    TfidfVectorizer    1000.0  0.8304             200     0.8208\n",
       "20    TfidfVectorizer       NaN  0.8512             200     0.8264"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vectorizer</th>\n",
       "      <th>features</th>\n",
       "      <th>score</th>\n",
       "      <th>pca_components</th>\n",
       "      <th>pca_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.7480</td>\n",
       "      <td>100</td>\n",
       "      <td>0.7480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.7948</td>\n",
       "      <td>200</td>\n",
       "      <td>0.7948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>300.0</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>200</td>\n",
       "      <td>0.7964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.8144</td>\n",
       "      <td>200</td>\n",
       "      <td>0.8036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>800.0</td>\n",
       "      <td>0.8184</td>\n",
       "      <td>200</td>\n",
       "      <td>0.8128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.8156</td>\n",
       "      <td>200</td>\n",
       "      <td>0.8084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8484</td>\n",
       "      <td>200</td>\n",
       "      <td>0.8184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HashingVectorizer</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.6652</td>\n",
       "      <td>100</td>\n",
       "      <td>0.6652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HashingVectorizer</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.7112</td>\n",
       "      <td>200</td>\n",
       "      <td>0.7116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>HashingVectorizer</td>\n",
       "      <td>300.0</td>\n",
       "      <td>0.7252</td>\n",
       "      <td>200</td>\n",
       "      <td>0.7204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>HashingVectorizer</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.7772</td>\n",
       "      <td>200</td>\n",
       "      <td>0.7632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>HashingVectorizer</td>\n",
       "      <td>800.0</td>\n",
       "      <td>0.7836</td>\n",
       "      <td>200</td>\n",
       "      <td>0.7752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>HashingVectorizer</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.7924</td>\n",
       "      <td>200</td>\n",
       "      <td>0.7748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>HashingVectorizer</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.8212</td>\n",
       "      <td>200</td>\n",
       "      <td>0.7956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.7504</td>\n",
       "      <td>100</td>\n",
       "      <td>0.7504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.7984</td>\n",
       "      <td>200</td>\n",
       "      <td>0.7984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>300.0</td>\n",
       "      <td>0.8020</td>\n",
       "      <td>200</td>\n",
       "      <td>0.8008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.8168</td>\n",
       "      <td>200</td>\n",
       "      <td>0.8088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>800.0</td>\n",
       "      <td>0.8228</td>\n",
       "      <td>200</td>\n",
       "      <td>0.8208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.8304</td>\n",
       "      <td>200</td>\n",
       "      <td>0.8208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8512</td>\n",
       "      <td>200</td>\n",
       "      <td>0.8264</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "list(result_df.groupby('features'))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(100.0,\n",
       "             vectorizer  features   score  pca_components  pca_score\n",
       "  0     CountVectorizer     100.0  0.7480             100     0.7480\n",
       "  7   HashingVectorizer     100.0  0.6652             100     0.6652\n",
       "  14    TfidfVectorizer     100.0  0.7504             100     0.7504),\n",
       " (200.0,\n",
       "             vectorizer  features   score  pca_components  pca_score\n",
       "  1     CountVectorizer     200.0  0.7948             200     0.7948\n",
       "  8   HashingVectorizer     200.0  0.7112             200     0.7116\n",
       "  15    TfidfVectorizer     200.0  0.7984             200     0.7984),\n",
       " (300.0,\n",
       "             vectorizer  features   score  pca_components  pca_score\n",
       "  2     CountVectorizer     300.0  0.8000             200     0.7964\n",
       "  9   HashingVectorizer     300.0  0.7252             200     0.7204\n",
       "  16    TfidfVectorizer     300.0  0.8020             200     0.8008),\n",
       " (500.0,\n",
       "             vectorizer  features   score  pca_components  pca_score\n",
       "  3     CountVectorizer     500.0  0.8144             200     0.8036\n",
       "  10  HashingVectorizer     500.0  0.7772             200     0.7632\n",
       "  17    TfidfVectorizer     500.0  0.8168             200     0.8088),\n",
       " (800.0,\n",
       "             vectorizer  features   score  pca_components  pca_score\n",
       "  4     CountVectorizer     800.0  0.8184             200     0.8128\n",
       "  11  HashingVectorizer     800.0  0.7836             200     0.7752\n",
       "  18    TfidfVectorizer     800.0  0.8228             200     0.8208),\n",
       " (1000.0,\n",
       "             vectorizer  features   score  pca_components  pca_score\n",
       "  5     CountVectorizer    1000.0  0.8156             200     0.8084\n",
       "  12  HashingVectorizer    1000.0  0.7924             200     0.7748\n",
       "  19    TfidfVectorizer    1000.0  0.8304             200     0.8208),\n",
       " (10000.0,\n",
       "             vectorizer  features   score  pca_components  pca_score\n",
       "  13  HashingVectorizer   10000.0  0.8212             200     0.7956)]"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Выводы:** Видно что с ростом к-ва фитчей качество обучения растет. \r\n",
    "В плане качества предсказания мета распределились в порядке убывания:\r\n",
    "1. TfidfVectorizer,\r\n",
    "2. CountVectorizer,\r\n",
    "3. HashingVectorizer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "==============================================================================="
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Дополнительные задания.(из другого файла). "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "4. Натренируем gensim.models.Word2Vec модель на наших данных.\n",
    "Тренировать будем на токенизированных твитах combine_df['tweet_token']\n",
    "Установим следующие параметры: size=200, window=5, min_count=2, sg = 1, hs = 0, negative = 10, workers= 32, seed = 34.\n",
    "Используем функцию train() с параметром total_examples равным длине combine_df['tweet_token'], количество epochs установим 20.\n",
    "\n",
    "\n",
    "5. Давайте немного потестируем нашу модель Word2Vec и посмотрим, как она работает. Мы зададим слово positive = \"dinner\", и модель вытащит из корпуса наиболее похожие слова c помощью функции most_similar. То же самое попробуем со словом \"trump\".\n",
    "\n",
    "\n",
    "6. Из приведенных выше примеров мы видим, что наша модель word2vec хорошо справляется с поиском наиболее похожих слов для данного слова. Но как она это делает? Она изучила векторы для каждого уникального слова наших данных и использует косинусное сходство, чтобы найти наиболее похожие векторы (слова).\n",
    "Давайте проверим векторное представление любого слова из нашего корпуса, например \"food\".\n",
    "\n",
    "\n",
    "7. Поскольку наши данные содержат твиты, а не только слова, нам придется придумать способ использовать векторы слов из модели word2vec для создания векторного представления всего твита. Существует простое решение этой проблемы, мы можем просто взять среднее значение всех векторов слов, присутствующих в твите. Длина результирующего вектора будет одинаковой, то есть 200. Мы повторим тот же процесс для всех твитов в наших данных и получим их векторы. Теперь у нас есть 200 функций word2vec для наших данных.\n",
    "Необходимо создать вектор для каждого твита, взяв среднее значение векторов слов, присутствующих в твите. В цикле сделать:  vec += model_w2v[word].reshape((1, size)) и поделить финальный вектор на количество слов в твите.\n",
    "\n",
    "На выходе должен получиться wordvec_df.shape = (49159, 200)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Решение:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "4. Натренируем **gensim.models.Word2Vec** модель на наших данных.\n",
    "\n",
    "Тренировать будем на токенизированных твитах combine_df['tweet_token']\n",
    "\n",
    "Установим следующие параметры: size=200, window=5, min_count=2, sg = 1, hs = 0, negative = 10, workers= 32, seed = 34.\n",
    "\n",
    "Используем функцию **train()** с параметром total_examples равным длине combine_df['tweet_token'], количество epochs установим 20."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "https://www.kaggle.com/pierremegret/gensim-word2vec-tutorial"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "source": [
    "# !pip install gensim"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "source": [
    "# !pip install python-Levenshtein"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "source": [
    "from gensim.models import Word2Vec"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "source": [
    "# sentences[0]\n",
    "data_train = df['tweet_token']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "\n",
    "class MyCorpus:\n",
    "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
    "\n",
    "    def __iter__(self):\n",
    "        corpus_path = datapath('lee_background.cor')\n",
    "        for line in open(corpus_path):\n",
    "            # assume there's one document per line, tokens separated by whitespace\n",
    "            yield utils.simple_preprocess(line)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\r\n",
    "The parameters:\r\n",
    "\r\n",
    "    min_count = int - Ignores all words with total absolute frequency lower than this - (2, 100)\r\n",
    "\r\n",
    "    window = int - The maximum distance between the current and predicted word within a sentence. E.g. window words on the left and window words on the left of our target - (2, 10)\r\n",
    "\r\n",
    "    size = int - Dimensionality of the feature vectors. - (50, 300)\r\n",
    "\r\n",
    "    sample = float - The threshold for configuring which higher-frequency words are randomly downsampled. Highly influencial. - (0, 1e-5)\r\n",
    "\r\n",
    "    alpha = float - The initial learning rate - (0.01, 0.05)\r\n",
    "\r\n",
    "    min_alpha = float - Learning rate will linearly drop to min_alpha as training progresses. To set it: alpha - (min_alpha * epochs) ~ 0.00\r\n",
    "\r\n",
    "    negative = int - If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\" should be drown. If set to 0, no negative sampling is used. - (5, 20)\r\n",
    "\r\n",
    "    workers = int - Use these many worker threads to train the model (=faster training with multicore machines)\r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "source": [
    "w2v_model = Word2Vec(#sentences=data_train, \n",
    "                    vector_size=200, \n",
    "                    window=5, \n",
    "                    min_count=2, \n",
    "                    sg = 1, \n",
    "                    hs = 0, \n",
    "                    negative = 10, \n",
    "                    workers= 32, \n",
    "                    seed = 34\n",
    "                   )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "source": [
    "from time import time  # To time our operations"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "source": [
    "t = time()\n",
    "\n",
    "w2v_model.build_vocab(data_train, progress_per=10000)\n",
    "\n",
    "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Time to build vocab: 0.01 mins\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "source": [
    "t = time()\n",
    "\n",
    "w2v_model.train(data_train, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Time to train the model: 0.88 mins\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "source": [
    "# найдем top-3 самых близких слов\n",
    "result = w2v_model.wv.similar_by_word(\"bihday\", topn=3)\n",
    "print(result)\n",
    "\n",
    "result = w2v_model.wv.similar_by_word(\"man\", topn=3)\n",
    "print(result)\n",
    "\n",
    "result = w2v_model.wv.similar_by_word(\"cat\", topn=3)\n",
    "print(result)\n",
    "\n",
    "result = w2v_model.wv.similar_by_word(\"mouth\", topn=3)\n",
    "print(result)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('bihay', 0.5713201761245728), ('bihdaycake', 0.5701730847358704), ('anniversaire', 0.5558146238327026)]\n",
      "[('cautious', 0.4471203088760376), ('gokaldas', 0.4262577295303345), ('hindu', 0.4240967333316803)]\n",
      "[('kitty', 0.5753360390663147), ('catsofinstagram', 0.5603662133216858), ('grumpycat', 0.5599806308746338)]\n",
      "[('endlessly', 0.5930461287498474), ('conormcgregor', 0.5641347765922546), ('godislove', 0.5115699768066406)]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "5. Давайте немного потестируем нашу модель Word2Vec и посмотрим, как она работает. Мы зададим слово positive = \"dinner\", и модель вытащит из корпуса наиболее похожие слова c помощью функции most_similar. То же самое попробуем со словом \"trump\"."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "source": [
    "w2v_model.wv.most_similar(positive=[\"dinner\"])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('bihdaydinner', 0.514960527420044),\n",
       " ('shawarma', 0.5066928267478943),\n",
       " ('cookout', 0.5062406063079834),\n",
       " ('bolognese', 0.5029006004333496),\n",
       " ('spaghetti', 0.49887171387672424),\n",
       " ('foodblogger', 0.4981864094734192),\n",
       " ('sizzle', 0.4914211630821228),\n",
       " ('tacotuesday', 0.48558303713798523),\n",
       " ('hamburger', 0.4844452142715454),\n",
       " ('sissy', 0.4799242317676544)]"
      ]
     },
     "metadata": {},
     "execution_count": 340
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "source": [
    "w2v_model.wv.most_similar(positive=[\"trump\"])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('donald', 0.5562459826469421),\n",
       " ('impeachment', 0.5360007286071777),\n",
       " ('suppoer', 0.5199249982833862),\n",
       " ('donaldtrump', 0.5138074159622192),\n",
       " ('dumptrump', 0.5079053044319153),\n",
       " ('racists', 0.5041542649269104),\n",
       " ('cuck', 0.5005506277084351),\n",
       " ('unfit', 0.49091798067092896),\n",
       " ('crony', 0.4903233051300049),\n",
       " ('fuhered', 0.4902151823043823)]"
      ]
     },
     "metadata": {},
     "execution_count": 341
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "6. Из приведенных выше примеров мы видим, что наша модель word2vec хорошо справляется с поиском наиболее похожих слов для данного слова. Но как она это делает? Она изучила векторы для каждого уникального слова наших данных и использует косинусное сходство, чтобы найти наиболее похожие векторы (слова).\n",
    "Давайте проверим векторное представление любого слова из нашего корпуса, например \"food\"."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "source": [
    "w2v_model.wv['food']"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ 9.72253382e-01, -5.32931387e-01, -1.97032005e-01, -3.45115751e-01,\n",
       "       -8.54220331e-01, -3.66696268e-01, -5.76752424e-01, -3.21528465e-01,\n",
       "       -3.21828604e-01, -1.38551265e-01, -5.17741218e-02,  1.44866526e-01,\n",
       "       -5.31215966e-01,  2.91604221e-01,  2.50220120e-01, -3.99498105e-01,\n",
       "        6.26124084e-01, -5.11294961e-01, -1.03406668e+00, -3.53472352e-01,\n",
       "       -4.64780629e-01,  4.70135100e-02, -1.01344299e+00, -7.69010335e-02,\n",
       "        8.42454806e-02,  4.17801142e-01, -5.29752970e-01, -4.05228995e-02,\n",
       "        7.05603004e-01, -4.64020576e-03,  6.14214182e-01, -4.81294423e-01,\n",
       "       -1.36772871e-01,  1.36004180e-01, -1.19141340e+00,  3.53754796e-02,\n",
       "        5.39124966e-01, -2.91756272e-01,  4.25235897e-01, -1.78252921e-01,\n",
       "       -4.23755720e-02, -1.91590205e-01, -2.74610192e-01,  3.51258308e-01,\n",
       "       -5.27885556e-01, -1.68294668e-01, -6.34512603e-01, -3.26083809e-01,\n",
       "       -5.06048024e-01, -3.28379840e-01,  9.41767275e-01,  5.38997293e-01,\n",
       "       -1.44808456e-01, -1.84355527e-01, -3.51146281e-01, -9.42150891e-01,\n",
       "        8.85018241e-03, -2.51311064e-01, -3.18747945e-02,  4.30810116e-02,\n",
       "       -5.60556650e-01,  6.44900426e-02, -3.60616259e-02, -1.92559212e-01,\n",
       "       -6.15307868e-01,  3.97911459e-01,  2.26928174e-01, -2.73096532e-01,\n",
       "        6.57253146e-01,  6.69758379e-01,  3.53521466e-01, -4.75638181e-01,\n",
       "        4.31182742e-01,  3.36420946e-02,  3.41476530e-01,  1.23141840e-01,\n",
       "       -3.64150703e-01,  3.41720283e-01, -5.20168960e-01, -7.45401800e-01,\n",
       "       -3.04663450e-01, -3.90482619e-02, -5.76923266e-02,  3.74138467e-02,\n",
       "        7.83797204e-02,  8.52598846e-01, -1.31315723e-01, -8.26227516e-02,\n",
       "       -2.27969941e-02,  3.02475065e-01,  8.06410253e-01, -5.34794986e-01,\n",
       "        2.86107033e-01,  7.78712407e-02,  5.53339161e-02, -2.50622898e-01,\n",
       "       -3.21171254e-01, -4.77906674e-01, -2.67703861e-01, -8.88032652e-03,\n",
       "        1.14095442e-01, -2.00366806e-02,  2.36661568e-01,  1.27991587e-01,\n",
       "       -8.68163884e-01,  6.82482064e-01,  2.91724712e-01, -7.27395833e-01,\n",
       "        3.82540852e-01, -8.32429230e-01, -8.04079235e-01,  4.82166260e-01,\n",
       "        3.51360232e-01, -1.30460203e+00,  1.82131901e-01, -9.70955566e-02,\n",
       "        3.78814518e-01, -2.57836848e-01,  5.00335276e-01,  7.12620378e-01,\n",
       "       -1.49300648e-02,  3.27050090e-02, -1.10579744e-01,  7.87227647e-04,\n",
       "        5.56750521e-02, -2.47563839e-01, -3.19435745e-01,  9.04559419e-02,\n",
       "        8.74784470e-01, -2.32156292e-01, -2.00051993e-01,  4.93484497e-01,\n",
       "       -1.72010362e-01,  2.83091366e-01,  1.67685077e-01, -8.87011662e-02,\n",
       "        6.43288046e-02, -1.80882469e-01,  4.78009492e-01, -1.33575588e-01,\n",
       "        2.90491786e-02,  3.97293091e-01,  4.34619069e-01, -4.33879048e-02,\n",
       "       -2.79124409e-01,  3.59349787e-01,  3.64197224e-01, -1.86771184e-01,\n",
       "        4.07617539e-01,  2.62603015e-01, -3.36728871e-01, -2.58738637e-01,\n",
       "        4.07241434e-01, -6.17702365e-01, -1.94442660e-01, -9.41309147e-03,\n",
       "        7.12981462e-01, -2.48367593e-01,  1.61117122e-01,  5.55433929e-01,\n",
       "        2.93270767e-01, -1.30530791e-02,  6.27225861e-02, -2.89606065e-01,\n",
       "        1.67813554e-01,  3.85993391e-01,  2.67851353e-01,  2.55666494e-01,\n",
       "       -1.45979270e-01, -9.72286612e-02, -7.19718218e-01, -2.72636086e-01,\n",
       "        7.83955634e-01,  3.36717606e-01, -5.93107283e-01,  3.81991357e-01,\n",
       "        1.85862586e-01,  4.32142168e-01,  2.85072755e-02, -6.58907533e-01,\n",
       "        2.29771554e-01,  2.04421598e-02,  1.05454159e+00, -5.53516507e-01,\n",
       "        8.58325779e-01,  3.25546116e-01,  2.87850350e-01,  3.51749957e-02,\n",
       "        8.09813678e-01, -1.63845748e-01,  9.02319476e-02,  2.88363785e-01,\n",
       "       -2.09749620e-02, -3.88982981e-01,  9.43416432e-02,  4.23815399e-02,\n",
       "        5.17832160e-01, -8.56182337e-01,  3.21429700e-01,  1.21385664e-01],\n",
       "      dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 345
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "7. Поскольку наши данные содержат твиты, а не только слова, нам придется придумать способ использовать векторы слов из модели word2vec для создания векторного представления всего твита. \n",
    "Существует простое решение этой проблемы, мы можем просто взять среднее значение всех векторов слов, присутствующих в твите.\n",
    "Длина результирующего вектора будет одинаковой, то есть 200. Мы повторим тот же процесс для всех твитов в наших данных и получим их векторы. \n",
    "Теперь у нас есть 200 функций word2vec для наших данных.\n",
    "Необходимо создать вектор для каждого твита, взяв среднее значение векторов слов, присутствующих в твите.\n",
    "В цикле сделать:  vec += model_w2v[word].reshape((1, size)) и поделить финальный вектор на количество слов в твите.\n",
    "\n",
    "На выходе должен получиться wordvec_df.shape = (49159, 200)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "source": [
    "def tweet_mean_vec_value(tweet_words):\n",
    "  \n",
    "    meen_values = []\n",
    "    word_count = len(tweet_words)\n",
    " # получим теперь векторы для нашего твита. \n",
    " # print(tweet_words)\n",
    "    tweet_vectors = []\n",
    "    for word in tweet_words:\n",
    "        try:\n",
    "            tweet_vectors.append(w2v_model.wv[word])\n",
    "        except Exception as ex:\n",
    "            word_count-=1\n",
    "            # print(str(ex))\n",
    "            \n",
    "    for i in range(200):\n",
    "        meen_value=0\n",
    "        for j in range(word_count):\n",
    "            meen_value += tweet_vectors[j][i]\n",
    "        if word_count>0:\n",
    "            meen_values.append(meen_value/word_count)\n",
    "    # print('======================')\n",
    "    if len(meen_values)>0:\n",
    "        return meen_values"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "source": [
    "tweets_vectors = list(map(tweet_mean_vec_value,df['tweet_token'].values))\n",
    "# перепишем функцию. для всех "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "np.array(tweets_vectors).shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(49159,)"
      ]
     },
     "metadata": {},
     "execution_count": 280
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "source": [
    "tweets = []\n",
    "nones = 0\n",
    "for t in tweets_vectors:\n",
    "    if t is None:\n",
    "        nones+=1\n",
    "    elif len(t) < 200:\n",
    "        print(len(t))\n",
    "    else:\n",
    "        tweets.append(t)\n",
    "        \n",
    "print('None_count=',nones)\n",
    "print(np.array(tweets).shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "None_count= 20\n",
      "(49139, 200)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Странно, но в некоторых витах есть слова для которых небыло создано вектора. \r\n",
    "Поэтому при ображении к такому слову возникает ошибка. \r\n",
    "При этом есть 20 твитов, ни для одного слова в которомнебыло создано вектора, таких твитов 20. это пустые массивы. \r\n",
    "Почему так вышло? "
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('imageai_env': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "interpreter": {
   "hash": "6b7c3ebca6b5b4a4b8af7b3624595a453b0710866dad47574474daaa4a66fc12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}