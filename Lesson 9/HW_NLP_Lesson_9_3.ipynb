{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Урок 9. Языковое моделирование\n",
    "\n",
    "## Задание\n",
    "Разобраться с моделькой генерации текста, собрать самим или взять датасет с вебинара и обучить генератор."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Решение"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "# The unique characters in the file\n",
    "import warnings # Не показывать предупреждения.\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "path_to_file = 'evgenyi_onegin.txt'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "\n",
    "# length of text is the number of characters in it\n",
    "print('Length of text: {} characters'.format(len(text)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Length of text: 286984 characters\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "print(text[:500])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Александр Сергеевич Пушкин\n",
      "\n",
      "                                Евгений Онегин\n",
      "                                Роман в стихах\n",
      "\n",
      "                        Не мысля гордый свет забавить,\n",
      "                        Вниманье дружбы возлюбя,\n",
      "                        Хотел бы я тебе представить\n",
      "                        Залог достойнее тебя,\n",
      "                        Достойнее души прекрасной,\n",
      "                        Святой исполненной мечты,\n",
      "                        Поэзии живой и ясной,\n",
      "                        Высо\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "text = text + text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "# Отсортируем все символы в датасете и удалим повтряющиеся. \n",
    "vocab = sorted(set(text))\n",
    "print('{} unique characters'.format(len(vocab)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "131 unique characters\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "#  Сделаем перевод номеров симоволов в текст. \n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "text_as_int, text[:30], len(text_as_int), len(text)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([ 71, 110, 104, ..., 104, 121,   0]),\n",
       " 'Александр Сергеевич Пушкин\\n\\n  ',\n",
       " 573968,\n",
       " 573968)"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### train and target"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "# The maximum length sentence you want for a single input in characters\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int) # ? Вот здесь не очень понятно. Зачем берут числа а не символы.\n",
    "\n",
    "for i in char_dataset.take(5):\n",
    "    print(idx2char[i.numpy()])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "А\n",
      "л\n",
      "е\n",
      "к\n",
      "с\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "char_dataset.take(5)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<TakeDataset shapes: (), types: tf.int64>"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "# Выполним разбиение текста на последовательности длинной 100 символов.\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "# Выведем первые 5 батчей на экран.\n",
    "for item in sequences.take(5):\n",
    "    print(repr(''.join(idx2char[item.numpy()])))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "'Александр Сергеевич Пушкин\\n\\n                                Евгений Онегин\\n                          '\n",
      "'      Роман в стихах\\n\\n                        Не мысля гордый свет забавить,\\n                        '\n",
      "'Вниманье дружбы возлюбя,\\n                        Хотел бы я тебе представить\\n                        '\n",
      "'Залог достойнее тебя,\\n                        Достойнее души прекрасной,\\n                        Свят'\n",
      "'ой исполненной мечты,\\n                        Поэзии живой и ясной,\\n                        Высоких д'\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1] # Предсказываем по одному символу.\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "dataset\n",
    "\n",
    "# Разделим его на тренировочную и тестовую выборки."
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<MapDataset shapes: ((100,), (100,)), types: (tf.int64, tf.int64)>"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Print the first example input and target values:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "    print('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "    print('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input data:  'Александр Сергеевич Пушкин\\n\\n                                Евгений Онегин\\n                         '\n",
      "Target data: 'лександр Сергеевич Пушкин\\n\\n                                Евгений Онегин\\n                          '\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 128\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "class RNNgenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_units): #, batch_size):\n",
    "        super(RNNgenerator, self).__init__()\n",
    "        \n",
    "        self.emb = tf.keras.layers.Embedding(vocab_size, embedding_dim) #, batch_input_shape=[batch_size, None])\n",
    "                                 \n",
    "        self.gru1 = tf.keras.layers.GRU(rnn_units,\n",
    "                            return_sequences=True,\n",
    "                            stateful=False,\n",
    "                            recurrent_initializer='glorot_uniform')\n",
    "\n",
    "        self.gru2 = tf.keras.layers.GRU(rnn_units,\n",
    "                            return_sequences=True,\n",
    "                            stateful=False,\n",
    "                            recurrent_initializer='glorot_uniform')\n",
    "        self.gru3 = tf.keras.layers.GRU(rnn_units,\n",
    "                            return_sequences=True,\n",
    "                            stateful=False,\n",
    "                            recurrent_initializer='glorot_uniform')\n",
    "        \n",
    "        # self.lin_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, x):\n",
    "        emb_x = self.emb(x)\n",
    "        x = self.gru1(emb_x)\n",
    "        x = self.gru2(x)\n",
    "        x = self.gru3(x)\n",
    "\n",
    "        x = self.dense(x)\n",
    "        return x \n",
    "\n",
    "model = RNNgenerator(vocab_size,\n",
    "                     embedding_dim,\n",
    "                     rnn_units=rnn_units\n",
    "                     ) #,\n",
    "                    #  batch_size= BATCH_SIZE)\n",
    "                     \n",
    "# model.build(tf.TensorShape([1, None]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "# model.summary()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "model.compile(optimizer='adam', loss=loss, metrics='accuracy')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Configure checkpoints"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './RNN_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "                                filepath=checkpoint_prefix,\n",
    "                                save_freq=5,\n",
    "                                save_weights_only=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Execute the training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "EPOCHS = 500"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "history = model.fit(dataset,\n",
    "                    epochs=EPOCHS,\n",
    "                    callbacks=[checkpoint_callback]\n",
    "                   )"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/500\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-09-16 22:00:00.553409: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
      "2021-09-16 22:00:07.075430: I tensorflow/stream_executor/cuda/cuda_dnn.cc:359] Loaded cuDNN version 8204\n",
      "2021-09-16 22:00:08.484064: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2021-09-16 22:00:14.219388: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "88/88 [==============================] - 94s 864ms/step - loss: 1.9484 - accuracy: 0.5623\n",
      "Epoch 2/500\n",
      "88/88 [==============================] - 73s 832ms/step - loss: 1.4290 - accuracy: 0.6061\n",
      "Epoch 3/500\n",
      "88/88 [==============================] - 73s 832ms/step - loss: 1.3043 - accuracy: 0.6296\n",
      "Epoch 4/500\n",
      "88/88 [==============================] - 74s 843ms/step - loss: 1.2095 - accuracy: 0.6517\n",
      "Epoch 5/500\n",
      "88/88 [==============================] - 74s 836ms/step - loss: 1.1170 - accuracy: 0.6737\n",
      "Epoch 6/500\n",
      "88/88 [==============================] - 72s 820ms/step - loss: 1.0212 - accuracy: 0.6991\n",
      "Epoch 7/500\n",
      "88/88 [==============================] - 74s 837ms/step - loss: 0.9607 - accuracy: 0.7209\n",
      "Epoch 8/500\n",
      "88/88 [==============================] - 72s 820ms/step - loss: 0.8615 - accuracy: 0.7461\n",
      "Epoch 9/500\n",
      "88/88 [==============================] - 74s 841ms/step - loss: 0.7663 - accuracy: 0.7721\n",
      "Epoch 10/500\n",
      "88/88 [==============================] - 75s 846ms/step - loss: 0.6989 - accuracy: 0.7986\n",
      "Epoch 11/500\n",
      "88/88 [==============================] - 72s 818ms/step - loss: 0.5830 - accuracy: 0.8322\n",
      "Epoch 12/500\n",
      "88/88 [==============================] - 74s 838ms/step - loss: 0.4923 - accuracy: 0.8650\n",
      "Epoch 13/500\n",
      "88/88 [==============================] - 72s 820ms/step - loss: 0.3953 - accuracy: 0.8953\n",
      "Epoch 14/500\n",
      "88/88 [==============================] - 75s 848ms/step - loss: 0.3309 - accuracy: 0.9137\n",
      "Epoch 15/500\n",
      "88/88 [==============================] - 74s 841ms/step - loss: 0.3012 - accuracy: 0.9217\n",
      "Epoch 16/500\n",
      "88/88 [==============================] - 72s 816ms/step - loss: 0.3107 - accuracy: 0.9221\n",
      "Epoch 17/500\n",
      "88/88 [==============================] - 74s 838ms/step - loss: 0.2581 - accuracy: 0.9299\n",
      "Epoch 18/500\n",
      "88/88 [==============================] - 73s 823ms/step - loss: 0.2361 - accuracy: 0.9347\n",
      "Epoch 19/500\n",
      "88/88 [==============================] - 75s 852ms/step - loss: 0.2522 - accuracy: 0.9333\n",
      "Epoch 20/500\n",
      "88/88 [==============================] - 75s 852ms/step - loss: 0.2170 - accuracy: 0.9400\n",
      "Epoch 21/500\n",
      "88/88 [==============================] - 74s 837ms/step - loss: 0.1986 - accuracy: 0.9451\n",
      "Epoch 22/500\n",
      "88/88 [==============================] - 75s 851ms/step - loss: 0.1850 - accuracy: 0.9497\n",
      "Epoch 23/500\n",
      "88/88 [==============================] - 73s 830ms/step - loss: 0.1793 - accuracy: 0.9520\n",
      "Epoch 24/500\n",
      "88/88 [==============================] - 76s 866ms/step - loss: 0.1845 - accuracy: 0.9498\n",
      "Epoch 25/500\n",
      "88/88 [==============================] - 77s 873ms/step - loss: 0.1549 - accuracy: 0.9594\n",
      "Epoch 26/500\n",
      "88/88 [==============================] - 73s 832ms/step - loss: 0.1436 - accuracy: 0.9629\n",
      "Epoch 27/500\n",
      "88/88 [==============================] - 75s 854ms/step - loss: 0.1348 - accuracy: 0.9655\n",
      "Epoch 28/500\n",
      "88/88 [==============================] - 73s 834ms/step - loss: 0.1277 - accuracy: 0.9676\n",
      "Epoch 29/500\n",
      "88/88 [==============================] - 76s 866ms/step - loss: 0.1222 - accuracy: 0.9688\n",
      "Epoch 30/500\n",
      "88/88 [==============================] - 76s 858ms/step - loss: 0.1189 - accuracy: 0.9700\n",
      "Epoch 31/500\n",
      "88/88 [==============================] - 73s 833ms/step - loss: 0.3528 - accuracy: 0.9154\n",
      "Epoch 32/500\n",
      "88/88 [==============================] - 78s 885ms/step - loss: 0.4231 - accuracy: 0.8764\n",
      "Epoch 33/500\n",
      "88/88 [==============================] - 74s 837ms/step - loss: 0.2100 - accuracy: 0.9422\n",
      "Epoch 34/500\n",
      "88/88 [==============================] - 74s 844ms/step - loss: 0.1402 - accuracy: 0.9644\n",
      "Epoch 35/500\n",
      "88/88 [==============================] - 76s 861ms/step - loss: 0.1188 - accuracy: 0.9703\n",
      "Epoch 36/500\n",
      "88/88 [==============================] - 72s 823ms/step - loss: 0.1096 - accuracy: 0.9723\n",
      "Epoch 37/500\n",
      "88/88 [==============================] - 73s 835ms/step - loss: 0.1045 - accuracy: 0.9734\n",
      "Epoch 38/500\n",
      "88/88 [==============================] - 72s 822ms/step - loss: 0.1016 - accuracy: 0.9739\n",
      "Epoch 39/500\n",
      "88/88 [==============================] - 80s 908ms/step - loss: 0.0994 - accuracy: 0.9743\n",
      "Epoch 40/500\n",
      "88/88 [==============================] - 80s 910ms/step - loss: 0.0981 - accuracy: 0.9745\n",
      "Epoch 41/500\n",
      "88/88 [==============================] - 77s 869ms/step - loss: 0.0967 - accuracy: 0.9748\n",
      "Epoch 42/500\n",
      "88/88 [==============================] - 81s 923ms/step - loss: 0.0958 - accuracy: 0.9750\n",
      "Epoch 43/500\n",
      "88/88 [==============================] - 82s 929ms/step - loss: 0.0947 - accuracy: 0.9753\n",
      "Epoch 44/500\n",
      "88/88 [==============================] - 84s 959ms/step - loss: 0.0941 - accuracy: 0.9755\n",
      "Epoch 45/500\n",
      "88/88 [==============================] - 73s 830ms/step - loss: 0.0936 - accuracy: 0.9755\n",
      "Epoch 46/500\n",
      "88/88 [==============================] - 72s 814ms/step - loss: 0.0929 - accuracy: 0.9759\n",
      "Epoch 47/500\n",
      "88/88 [==============================] - 73s 824ms/step - loss: 0.0930 - accuracy: 0.9758\n",
      "Epoch 48/500\n",
      "88/88 [==============================] - 73s 826ms/step - loss: 0.0921 - accuracy: 0.9759\n",
      "Epoch 49/500\n",
      "88/88 [==============================] - 72s 822ms/step - loss: 0.0913 - accuracy: 0.9762\n",
      "Epoch 50/500\n",
      "88/88 [==============================] - 73s 826ms/step - loss: 0.0905 - accuracy: 0.9765\n",
      "Epoch 51/500\n",
      "88/88 [==============================] - 72s 820ms/step - loss: 0.0897 - accuracy: 0.9769\n",
      "Epoch 52/500\n",
      "88/88 [==============================] - 73s 832ms/step - loss: 0.0893 - accuracy: 0.9771\n",
      "Epoch 53/500\n",
      "88/88 [==============================] - 72s 815ms/step - loss: 0.0884 - accuracy: 0.9774\n",
      "Epoch 54/500\n",
      "88/88 [==============================] - 74s 840ms/step - loss: 0.0883 - accuracy: 0.9775\n",
      "Epoch 55/500\n",
      "88/88 [==============================] - 73s 829ms/step - loss: 0.0870 - accuracy: 0.9779\n",
      "Epoch 56/500\n",
      "88/88 [==============================] - 71s 808ms/step - loss: 0.0872 - accuracy: 0.9779\n",
      "Epoch 57/500\n",
      "88/88 [==============================] - 74s 840ms/step - loss: 0.0876 - accuracy: 0.9779\n",
      "Epoch 58/500\n",
      "88/88 [==============================] - 71s 807ms/step - loss: 0.0879 - accuracy: 0.9778\n",
      "Epoch 59/500\n",
      "88/88 [==============================] - 74s 845ms/step - loss: 0.0903 - accuracy: 0.9775\n",
      "Epoch 60/500\n",
      "88/88 [==============================] - 74s 841ms/step - loss: 0.1084 - accuracy: 0.9723\n",
      "Epoch 61/500\n",
      "88/88 [==============================] - 72s 822ms/step - loss: 0.2864 - accuracy: 0.9132\n",
      "Epoch 62/500\n",
      "88/88 [==============================] - 74s 844ms/step - loss: 0.3284 - accuracy: 0.8978\n",
      "Epoch 63/500\n",
      "88/88 [==============================] - 72s 822ms/step - loss: 0.2042 - accuracy: 0.9393\n",
      "Epoch 64/500\n",
      "88/88 [==============================] - 73s 830ms/step - loss: 0.1342 - accuracy: 0.9643\n",
      "Epoch 65/500\n",
      "88/88 [==============================] - 75s 851ms/step - loss: 0.1040 - accuracy: 0.9739\n",
      "Epoch 66/500\n",
      "88/88 [==============================] - 72s 812ms/step - loss: 0.0909 - accuracy: 0.9774\n",
      "Epoch 67/500\n",
      "88/88 [==============================] - 75s 851ms/step - loss: 0.0853 - accuracy: 0.9786\n",
      "Epoch 68/500\n",
      "88/88 [==============================] - 71s 811ms/step - loss: 0.0828 - accuracy: 0.9788\n",
      "Epoch 69/500\n",
      "88/88 [==============================] - 74s 842ms/step - loss: 0.0815 - accuracy: 0.9791\n",
      "Epoch 70/500\n",
      "88/88 [==============================] - 74s 837ms/step - loss: 0.0804 - accuracy: 0.9791\n",
      "Epoch 71/500\n",
      "88/88 [==============================] - 73s 826ms/step - loss: 0.0801 - accuracy: 0.9791\n",
      "Epoch 72/500\n",
      "88/88 [==============================] - 74s 840ms/step - loss: 0.0799 - accuracy: 0.9790\n",
      "Epoch 73/500\n",
      "88/88 [==============================] - 73s 823ms/step - loss: 0.0792 - accuracy: 0.9792\n",
      "Epoch 74/500\n",
      "88/88 [==============================] - 75s 850ms/step - loss: 0.0793 - accuracy: 0.9791\n",
      "Epoch 75/500\n",
      "88/88 [==============================] - 75s 853ms/step - loss: 0.0790 - accuracy: 0.9791\n",
      "Epoch 76/500\n",
      "88/88 [==============================] - 74s 842ms/step - loss: 0.0791 - accuracy: 0.9790\n",
      "Epoch 77/500\n",
      "88/88 [==============================] - 76s 859ms/step - loss: 0.0790 - accuracy: 0.9791\n",
      "Epoch 78/500\n",
      "88/88 [==============================] - 79s 893ms/step - loss: 0.0789 - accuracy: 0.9791\n",
      "Epoch 79/500\n",
      "88/88 [==============================] - 76s 862ms/step - loss: 0.0788 - accuracy: 0.9791\n",
      "Epoch 80/500\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "88/88 [==============================] - 74s 838ms/step - loss: 0.0793 - accuracy: 0.9791\n",
      "Epoch 81/500\n",
      "88/88 [==============================] - 72s 822ms/step - loss: 0.0792 - accuracy: 0.9791\n",
      "Epoch 82/500\n",
      "88/88 [==============================] - 75s 853ms/step - loss: 0.0796 - accuracy: 0.9790\n",
      "Epoch 83/500\n",
      "88/88 [==============================] - 70s 799ms/step - loss: 0.0793 - accuracy: 0.9791\n",
      "Epoch 84/500\n",
      "88/88 [==============================] - 75s 848ms/step - loss: 0.0796 - accuracy: 0.9791\n",
      "Epoch 85/500\n",
      "88/88 [==============================] - 78s 881ms/step - loss: 0.0795 - accuracy: 0.9791\n",
      "Epoch 86/500\n",
      "88/88 [==============================] - 76s 868ms/step - loss: 0.0796 - accuracy: 0.9791\n",
      "Epoch 87/500\n",
      "88/88 [==============================] - 81s 926ms/step - loss: 0.0802 - accuracy: 0.9788\n",
      "Epoch 88/500\n",
      "88/88 [==============================] - 79s 893ms/step - loss: 0.0809 - accuracy: 0.9788\n",
      "Epoch 89/500\n",
      "88/88 [==============================] - 82s 932ms/step - loss: 0.0824 - accuracy: 0.9786\n",
      "Epoch 90/500\n",
      "88/88 [==============================] - 77s 870ms/step - loss: 0.0868 - accuracy: 0.9779\n",
      "Epoch 91/500\n",
      "88/88 [==============================] - 71s 807ms/step - loss: 0.2318 - accuracy: 0.9359\n",
      "Epoch 92/500\n",
      "88/88 [==============================] - 74s 843ms/step - loss: 1.4004 - accuracy: 0.6999\n",
      "Epoch 93/500\n",
      "88/88 [==============================] - 72s 812ms/step - loss: 0.8113 - accuracy: 0.7708\n",
      "Epoch 94/500\n",
      "88/88 [==============================] - 72s 823ms/step - loss: 0.5694 - accuracy: 0.8291\n",
      "Epoch 95/500\n",
      "88/88 [==============================] - 74s 837ms/step - loss: 0.4204 - accuracy: 0.8763\n",
      "Epoch 96/500\n",
      "88/88 [==============================] - 72s 813ms/step - loss: 0.3174 - accuracy: 0.9108\n",
      "Epoch 97/500\n",
      "88/88 [==============================] - 76s 866ms/step - loss: 0.2477 - accuracy: 0.9339\n",
      "Epoch 98/500\n",
      "88/88 [==============================] - 74s 839ms/step - loss: 0.2062 - accuracy: 0.9460\n",
      "Epoch 99/500\n",
      "88/88 [==============================] - 74s 843ms/step - loss: 0.1676 - accuracy: 0.9578\n",
      "Epoch 100/500\n",
      "88/88 [==============================] - 77s 880ms/step - loss: 0.1733 - accuracy: 0.9565\n",
      "Epoch 101/500\n",
      "88/88 [==============================] - 86s 973ms/step - loss: 0.1428 - accuracy: 0.9643\n",
      "Epoch 102/500\n",
      "88/88 [==============================] - 74s 841ms/step - loss: 0.1230 - accuracy: 0.9699\n",
      "Epoch 103/500\n",
      "88/88 [==============================] - 72s 816ms/step - loss: 0.1141 - accuracy: 0.9722\n",
      "Epoch 104/500\n",
      "88/88 [==============================] - 74s 843ms/step - loss: 0.1262 - accuracy: 0.9682\n",
      "Epoch 105/500\n",
      "88/88 [==============================] - 74s 842ms/step - loss: 0.1232 - accuracy: 0.9688\n",
      "Epoch 106/500\n",
      "88/88 [==============================] - 71s 811ms/step - loss: 0.1160 - accuracy: 0.9710\n",
      "Epoch 107/500\n",
      "88/88 [==============================] - 74s 842ms/step - loss: 0.1166 - accuracy: 0.9702\n",
      "Epoch 108/500\n",
      "88/88 [==============================] - 73s 827ms/step - loss: 0.1028 - accuracy: 0.9740\n",
      "Epoch 109/500\n",
      "88/88 [==============================] - 75s 852ms/step - loss: 0.0989 - accuracy: 0.9747\n",
      "Epoch 110/500\n",
      "88/88 [==============================] - 74s 839ms/step - loss: 0.1516 - accuracy: 0.9635\n",
      "Epoch 111/500\n",
      "88/88 [==============================] - 73s 826ms/step - loss: 0.1192 - accuracy: 0.9704\n",
      "Epoch 112/500\n",
      "88/88 [==============================] - 74s 842ms/step - loss: 0.1344 - accuracy: 0.9663\n",
      "Epoch 113/500\n",
      "88/88 [==============================] - 72s 816ms/step - loss: 0.1317 - accuracy: 0.9670\n",
      "Epoch 114/500\n",
      "88/88 [==============================] - 75s 847ms/step - loss: 0.1248 - accuracy: 0.9685\n",
      "Epoch 115/500\n",
      "88/88 [==============================] - 74s 846ms/step - loss: 0.1113 - accuracy: 0.9723\n",
      "Epoch 116/500\n",
      "88/88 [==============================] - 73s 833ms/step - loss: 0.1038 - accuracy: 0.9739\n",
      "Epoch 117/500\n",
      "88/88 [==============================] - 75s 847ms/step - loss: 0.1131 - accuracy: 0.9706\n",
      "Epoch 118/500\n",
      "88/88 [==============================] - 73s 826ms/step - loss: 0.1176 - accuracy: 0.9694\n",
      "Epoch 119/500\n",
      "88/88 [==============================] - 75s 849ms/step - loss: 0.1061 - accuracy: 0.9733\n",
      "Epoch 120/500\n",
      "88/88 [==============================] - 76s 859ms/step - loss: 0.1214 - accuracy: 0.9690\n",
      "Epoch 121/500\n",
      "88/88 [==============================] - 75s 848ms/step - loss: 0.1121 - accuracy: 0.9717\n",
      "Epoch 122/500\n",
      "88/88 [==============================] - 76s 858ms/step - loss: 0.1071 - accuracy: 0.9730\n",
      "Epoch 123/500\n",
      "88/88 [==============================] - 72s 823ms/step - loss: 0.1048 - accuracy: 0.9735\n",
      "Epoch 124/500\n",
      "88/88 [==============================] - 74s 842ms/step - loss: 0.1032 - accuracy: 0.9738\n",
      "Epoch 125/500\n",
      "88/88 [==============================] - 76s 858ms/step - loss: 0.1010 - accuracy: 0.9744\n",
      "Epoch 126/500\n",
      "88/88 [==============================] - 72s 813ms/step - loss: 0.0988 - accuracy: 0.9748\n",
      "Epoch 127/500\n",
      "88/88 [==============================] - 74s 844ms/step - loss: 0.1035 - accuracy: 0.9737\n",
      "Epoch 128/500\n",
      "88/88 [==============================] - 73s 832ms/step - loss: 0.1783 - accuracy: 0.9498\n",
      "Epoch 129/500\n",
      "88/88 [==============================] - 80s 911ms/step - loss: 0.4450 - accuracy: 0.8669\n",
      "Epoch 130/500\n",
      "88/88 [==============================] - 78s 885ms/step - loss: 0.5508 - accuracy: 0.8369\n",
      "Epoch 131/500\n",
      "88/88 [==============================] - 78s 882ms/step - loss: 0.4465 - accuracy: 0.8630\n",
      "Epoch 132/500\n",
      "88/88 [==============================] - 83s 947ms/step - loss: 0.3309 - accuracy: 0.8970\n",
      "Epoch 133/500\n",
      "88/88 [==============================] - 71s 810ms/step - loss: 0.2701 - accuracy: 0.9192\n",
      "Epoch 134/500\n",
      "88/88 [==============================] - 73s 825ms/step - loss: 0.2128 - accuracy: 0.9375\n",
      "Epoch 135/500\n",
      "88/88 [==============================] - 73s 831ms/step - loss: 0.1666 - accuracy: 0.9540\n",
      "Epoch 136/500\n",
      "88/88 [==============================] - 72s 813ms/step - loss: 0.1368 - accuracy: 0.9645\n",
      "Epoch 137/500\n",
      "88/88 [==============================] - 74s 837ms/step - loss: 0.1159 - accuracy: 0.9711\n",
      "Epoch 138/500\n",
      "88/88 [==============================] - 71s 805ms/step - loss: 0.1023 - accuracy: 0.9748\n",
      "Epoch 139/500\n",
      "88/88 [==============================] - 75s 858ms/step - loss: 0.0945 - accuracy: 0.9763\n",
      "Epoch 140/500\n",
      "88/88 [==============================] - 75s 849ms/step - loss: 0.1354 - accuracy: 0.9670\n",
      "Epoch 141/500\n",
      "88/88 [==============================] - 72s 817ms/step - loss: 0.0991 - accuracy: 0.9751\n",
      "Epoch 142/500\n",
      "88/88 [==============================] - 73s 833ms/step - loss: 0.0908 - accuracy: 0.9772\n",
      "Epoch 143/500\n",
      "88/88 [==============================] - 82s 928ms/step - loss: 0.0878 - accuracy: 0.9773\n",
      "Epoch 144/500\n",
      "88/88 [==============================] - 75s 847ms/step - loss: 0.0857 - accuracy: 0.9777\n",
      "Epoch 145/500\n",
      "88/88 [==============================] - 77s 871ms/step - loss: 0.0843 - accuracy: 0.9779\n",
      "Epoch 146/500\n",
      "88/88 [==============================] - 72s 814ms/step - loss: 0.0828 - accuracy: 0.9780\n",
      "Epoch 147/500\n",
      "88/88 [==============================] - 76s 860ms/step - loss: 0.0819 - accuracy: 0.9782\n",
      "Epoch 148/500\n",
      "88/88 [==============================] - 72s 820ms/step - loss: 0.0815 - accuracy: 0.9783\n",
      "Epoch 149/500\n",
      "88/88 [==============================] - 75s 846ms/step - loss: 0.0813 - accuracy: 0.9782\n",
      "Epoch 150/500\n",
      "88/88 [==============================] - 75s 850ms/step - loss: 0.1004 - accuracy: 0.9735\n",
      "Epoch 151/500\n",
      "88/88 [==============================] - 72s 818ms/step - loss: 0.1410 - accuracy: 0.9604\n",
      "Epoch 152/500\n",
      "88/88 [==============================] - 74s 844ms/step - loss: 0.2099 - accuracy: 0.9364\n",
      "Epoch 153/500\n",
      "88/88 [==============================] - 72s 821ms/step - loss: 0.3387 - accuracy: 0.8954\n",
      "Epoch 154/500\n",
      "88/88 [==============================] - 73s 834ms/step - loss: 0.4215 - accuracy: 0.8734\n",
      "Epoch 155/500\n",
      "88/88 [==============================] - 74s 841ms/step - loss: 0.3837 - accuracy: 0.8817\n",
      "Epoch 156/500\n",
      "88/88 [==============================] - 72s 818ms/step - loss: 0.3428 - accuracy: 0.8950\n",
      "Epoch 157/500\n",
      "88/88 [==============================] - 76s 865ms/step - loss: 0.2681 - accuracy: 0.9169\n",
      "Epoch 158/500\n",
      "88/88 [==============================] - 72s 819ms/step - loss: 0.2222 - accuracy: 0.9322\n",
      "Epoch 159/500\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "88/88 [==============================] - 76s 860ms/step - loss: 0.1863 - accuracy: 0.9448\n",
      "Epoch 160/500\n",
      "88/88 [==============================] - 74s 845ms/step - loss: 0.1557 - accuracy: 0.9557\n",
      "Epoch 161/500\n",
      "88/88 [==============================] - 74s 837ms/step - loss: 0.1342 - accuracy: 0.9639\n",
      "Epoch 162/500\n",
      "88/88 [==============================] - 77s 872ms/step - loss: 0.1176 - accuracy: 0.9696\n",
      "Epoch 163/500\n",
      "88/88 [==============================] - 74s 841ms/step - loss: 0.1045 - accuracy: 0.9739\n",
      "Epoch 164/500\n",
      "88/88 [==============================] - 73s 832ms/step - loss: 0.0952 - accuracy: 0.9765\n",
      "Epoch 165/500\n",
      "88/88 [==============================] - 75s 850ms/step - loss: 0.1004 - accuracy: 0.9759\n",
      "Epoch 166/500\n",
      "88/88 [==============================] - 73s 828ms/step - loss: 0.1064 - accuracy: 0.9732\n",
      "Epoch 167/500\n",
      "88/88 [==============================] - 73s 829ms/step - loss: 0.0896 - accuracy: 0.9774\n",
      "Epoch 168/500\n",
      "88/88 [==============================] - 74s 835ms/step - loss: 0.0852 - accuracy: 0.9781\n",
      "Epoch 169/500\n",
      "88/88 [==============================] - 80s 905ms/step - loss: 0.0830 - accuracy: 0.9784\n",
      "Epoch 170/500\n",
      "88/88 [==============================] - 78s 885ms/step - loss: 0.0817 - accuracy: 0.9784\n",
      "Epoch 171/500\n",
      "88/88 [==============================] - 77s 877ms/step - loss: 0.0807 - accuracy: 0.9786\n",
      "Epoch 172/500\n",
      "88/88 [==============================] - 81s 925ms/step - loss: 0.0799 - accuracy: 0.9785\n",
      "Epoch 173/500\n",
      "88/88 [==============================] - 74s 836ms/step - loss: 0.0796 - accuracy: 0.9786\n",
      "Epoch 174/500\n",
      "88/88 [==============================] - 72s 822ms/step - loss: 0.0795 - accuracy: 0.9787\n",
      "Epoch 175/500\n",
      "88/88 [==============================] - 73s 833ms/step - loss: 0.0792 - accuracy: 0.9786\n",
      "Epoch 176/500\n",
      "88/88 [==============================] - 70s 796ms/step - loss: 0.0790 - accuracy: 0.9787\n",
      "Epoch 177/500\n",
      "88/88 [==============================] - 74s 835ms/step - loss: 0.0787 - accuracy: 0.9788\n",
      "Epoch 178/500\n",
      "88/88 [==============================] - 72s 812ms/step - loss: 0.0785 - accuracy: 0.9787\n",
      "Epoch 179/500\n",
      "88/88 [==============================] - 73s 824ms/step - loss: 0.0789 - accuracy: 0.9785\n",
      "Epoch 180/500\n",
      "88/88 [==============================] - 74s 836ms/step - loss: 0.2931 - accuracy: 0.9173\n",
      "Epoch 181/500\n",
      "88/88 [==============================] - 71s 806ms/step - loss: 0.6496 - accuracy: 0.8204\n",
      "Epoch 182/500\n",
      "88/88 [==============================] - 78s 884ms/step - loss: 0.7092 - accuracy: 0.8018\n",
      "Epoch 183/500\n",
      "88/88 [==============================] - 78s 884ms/step - loss: 0.6051 - accuracy: 0.8205\n",
      "Epoch 184/500\n",
      "88/88 [==============================] - 73s 828ms/step - loss: 0.5019 - accuracy: 0.8452\n",
      "Epoch 185/500\n",
      "88/88 [==============================] - 75s 849ms/step - loss: 0.4258 - accuracy: 0.8670\n",
      "Epoch 186/500\n",
      "88/88 [==============================] - 71s 810ms/step - loss: 0.3529 - accuracy: 0.8892\n",
      "Epoch 187/500\n",
      "88/88 [==============================] - 74s 838ms/step - loss: 0.3062 - accuracy: 0.9049\n",
      "Epoch 188/500\n",
      "88/88 [==============================] - 73s 824ms/step - loss: 0.2651 - accuracy: 0.9193\n",
      "Epoch 189/500\n",
      "88/88 [==============================] - 75s 851ms/step - loss: 0.2320 - accuracy: 0.9308\n",
      "Epoch 190/500\n",
      "88/88 [==============================] - 74s 839ms/step - loss: 0.2052 - accuracy: 0.9407\n",
      "Epoch 191/500\n",
      "88/88 [==============================] - 73s 826ms/step - loss: 0.1832 - accuracy: 0.9483\n",
      "Epoch 192/500\n",
      "88/88 [==============================] - 76s 859ms/step - loss: 0.1664 - accuracy: 0.9545\n",
      "Epoch 193/500\n",
      "88/88 [==============================] - 72s 822ms/step - loss: 0.1497 - accuracy: 0.9601\n",
      "Epoch 194/500\n",
      "88/88 [==============================] - 75s 855ms/step - loss: 0.1361 - accuracy: 0.9649\n",
      "Epoch 195/500\n",
      "88/88 [==============================] - 74s 842ms/step - loss: 0.1231 - accuracy: 0.9693\n",
      "Epoch 196/500\n",
      "88/88 [==============================] - 72s 820ms/step - loss: 0.1125 - accuracy: 0.9726\n",
      "Epoch 197/500\n",
      "88/88 [==============================] - 79s 894ms/step - loss: 0.1035 - accuracy: 0.9751\n",
      "Epoch 198/500\n",
      "88/88 [==============================] - 84s 954ms/step - loss: 0.0964 - accuracy: 0.9764\n",
      "Epoch 199/500\n",
      "88/88 [==============================] - 75s 857ms/step - loss: 0.0908 - accuracy: 0.9777\n",
      "Epoch 200/500\n",
      "88/88 [==============================] - 76s 864ms/step - loss: 0.0876 - accuracy: 0.9781\n",
      "Epoch 201/500\n",
      "88/88 [==============================] - 73s 828ms/step - loss: 0.0849 - accuracy: 0.9784\n",
      "Epoch 202/500\n",
      "88/88 [==============================] - 74s 843ms/step - loss: 0.0835 - accuracy: 0.9785\n",
      "Epoch 203/500\n",
      "88/88 [==============================] - 72s 821ms/step - loss: 0.1241 - accuracy: 0.9692\n",
      "Epoch 204/500\n",
      "88/88 [==============================] - 76s 859ms/step - loss: 0.1299 - accuracy: 0.9648\n",
      "Epoch 205/500\n",
      "88/88 [==============================] - 73s 828ms/step - loss: 0.2234 - accuracy: 0.9383\n",
      "Epoch 206/500\n",
      "88/88 [==============================] - 74s 837ms/step - loss: 0.3513 - accuracy: 0.8920\n",
      "Epoch 207/500\n",
      "88/88 [==============================] - 78s 883ms/step - loss: 0.4494 - accuracy: 0.8635\n",
      "Epoch 208/500\n",
      "88/88 [==============================] - 76s 858ms/step - loss: 0.5020 - accuracy: 0.8489\n",
      "Epoch 209/500\n",
      "88/88 [==============================] - 78s 882ms/step - loss: 0.4758 - accuracy: 0.8543\n",
      "Epoch 210/500\n",
      "88/88 [==============================] - 81s 924ms/step - loss: 0.4550 - accuracy: 0.8595\n",
      "Epoch 211/500\n",
      "88/88 [==============================] - 74s 837ms/step - loss: 0.4198 - accuracy: 0.8690\n",
      "Epoch 212/500\n",
      "88/88 [==============================] - 74s 839ms/step - loss: 0.3805 - accuracy: 0.8799\n",
      "Epoch 213/500\n",
      "88/88 [==============================] - 71s 804ms/step - loss: 0.3603 - accuracy: 0.8866\n",
      "Epoch 214/500\n",
      "88/88 [==============================] - 73s 832ms/step - loss: 0.3287 - accuracy: 0.8965\n",
      "Epoch 215/500\n",
      "88/88 [==============================] - 74s 836ms/step - loss: 0.3074 - accuracy: 0.9037\n",
      "Epoch 216/500\n",
      "88/88 [==============================] - 71s 811ms/step - loss: 0.2815 - accuracy: 0.9121\n",
      "Epoch 217/500\n",
      "88/88 [==============================] - 74s 836ms/step - loss: 0.2718 - accuracy: 0.9163\n",
      "Epoch 218/500\n",
      "88/88 [==============================] - 72s 819ms/step - loss: 0.2496 - accuracy: 0.9233\n",
      "Epoch 219/500\n",
      "88/88 [==============================] - 75s 847ms/step - loss: 0.2317 - accuracy: 0.9292\n",
      "Epoch 220/500\n",
      "88/88 [==============================] - 79s 896ms/step - loss: 0.2198 - accuracy: 0.9337\n",
      "Epoch 221/500\n",
      "88/88 [==============================] - 75s 854ms/step - loss: 0.2024 - accuracy: 0.9399\n",
      "Epoch 222/500\n",
      "88/88 [==============================] - 74s 836ms/step - loss: 0.2261 - accuracy: 0.9329\n",
      "Epoch 223/500\n",
      "88/88 [==============================] - 72s 821ms/step - loss: 0.1999 - accuracy: 0.9405\n",
      "Epoch 224/500\n",
      "88/88 [==============================] - 73s 833ms/step - loss: 0.1898 - accuracy: 0.9443\n",
      "Epoch 225/500\n",
      "88/88 [==============================] - 75s 847ms/step - loss: 0.1823 - accuracy: 0.9468\n",
      "Epoch 226/500\n",
      "88/88 [==============================] - 71s 806ms/step - loss: 0.1794 - accuracy: 0.9478\n",
      "Epoch 227/500\n",
      "88/88 [==============================] - 75s 847ms/step - loss: 0.1734 - accuracy: 0.9496\n",
      "Epoch 228/500\n",
      "88/88 [==============================] - 72s 820ms/step - loss: 0.1703 - accuracy: 0.9507\n",
      "Epoch 229/500\n",
      "88/88 [==============================] - 75s 848ms/step - loss: 0.1649 - accuracy: 0.9526\n",
      "Epoch 230/500\n",
      "88/88 [==============================] - 74s 839ms/step - loss: 0.1631 - accuracy: 0.9531\n",
      "Epoch 231/500\n",
      "88/88 [==============================] - 72s 820ms/step - loss: 0.1614 - accuracy: 0.9538\n",
      "Epoch 232/500\n",
      "88/88 [==============================] - 74s 845ms/step - loss: 0.1638 - accuracy: 0.9526\n",
      "Epoch 233/500\n",
      "88/88 [==============================] - 76s 858ms/step - loss: 0.1725 - accuracy: 0.9493\n",
      "Epoch 234/500\n",
      "88/88 [==============================] - 79s 894ms/step - loss: 0.1822 - accuracy: 0.9460\n",
      "Epoch 235/500\n",
      "88/88 [==============================] - 80s 913ms/step - loss: 0.2058 - accuracy: 0.9382\n",
      "Epoch 236/500\n",
      "88/88 [==============================] - 74s 841ms/step - loss: 0.2394 - accuracy: 0.9261\n",
      "Epoch 237/500\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "88/88 [==============================] - 76s 866ms/step - loss: 0.2559 - accuracy: 0.9203\n",
      "Epoch 238/500\n",
      "88/88 [==============================] - 73s 826ms/step - loss: 0.2964 - accuracy: 0.9078\n",
      "Epoch 239/500\n",
      "88/88 [==============================] - 74s 839ms/step - loss: 0.3262 - accuracy: 0.8976\n",
      "Epoch 240/500\n",
      "88/88 [==============================] - 76s 864ms/step - loss: 0.3793 - accuracy: 0.8826\n",
      "Epoch 241/500\n",
      "88/88 [==============================] - 73s 827ms/step - loss: 0.4286 - accuracy: 0.8687\n",
      "Epoch 242/500\n",
      "88/88 [==============================] - 75s 852ms/step - loss: 0.4329 - accuracy: 0.8665\n",
      "Epoch 243/500\n",
      "88/88 [==============================] - 75s 854ms/step - loss: 0.4549 - accuracy: 0.8598\n",
      "Epoch 244/500\n",
      "88/88 [==============================] - 78s 883ms/step - loss: 0.4394 - accuracy: 0.8643\n",
      "Epoch 245/500\n",
      "88/88 [==============================] - 77s 877ms/step - loss: 0.4324 - accuracy: 0.8650\n",
      "Epoch 246/500\n",
      "88/88 [==============================] - 76s 858ms/step - loss: 0.4522 - accuracy: 0.8595\n",
      "Epoch 247/500\n",
      "88/88 [==============================] - 77s 871ms/step - loss: 0.4277 - accuracy: 0.8662\n",
      "Epoch 248/500\n",
      "88/88 [==============================] - 70s 799ms/step - loss: 0.3882 - accuracy: 0.8777\n",
      "Epoch 249/500\n",
      "88/88 [==============================] - 73s 831ms/step - loss: 0.3907 - accuracy: 0.8765\n",
      "Epoch 250/500\n",
      "88/88 [==============================] - 73s 830ms/step - loss: 0.3796 - accuracy: 0.8797\n",
      "Epoch 251/500\n",
      "88/88 [==============================] - 73s 829ms/step - loss: 0.3734 - accuracy: 0.8815\n",
      "Epoch 252/500\n",
      "88/88 [==============================] - 74s 842ms/step - loss: 0.3513 - accuracy: 0.8884\n",
      "Epoch 253/500\n",
      "88/88 [==============================] - 71s 809ms/step - loss: 0.3422 - accuracy: 0.8912\n",
      "Epoch 254/500\n",
      "88/88 [==============================] - 74s 844ms/step - loss: 0.3308 - accuracy: 0.8947\n",
      "Epoch 255/500\n",
      "88/88 [==============================] - 76s 865ms/step - loss: 0.3334 - accuracy: 0.8942\n",
      "Epoch 256/500\n",
      "88/88 [==============================] - 78s 884ms/step - loss: 0.3319 - accuracy: 0.8948\n",
      "Epoch 257/500\n",
      "88/88 [==============================] - 75s 850ms/step - loss: 0.3247 - accuracy: 0.8973\n",
      "Epoch 258/500\n",
      "88/88 [==============================] - 72s 822ms/step - loss: 0.3273 - accuracy: 0.8972\n",
      "Epoch 259/500\n",
      "88/88 [==============================] - 74s 839ms/step - loss: 0.3207 - accuracy: 0.8986\n",
      "Epoch 260/500\n",
      "88/88 [==============================] - 75s 851ms/step - loss: 0.3322 - accuracy: 0.8961\n",
      "Epoch 261/500\n",
      "88/88 [==============================] - 75s 851ms/step - loss: 0.3357 - accuracy: 0.8939\n",
      "Epoch 262/500\n",
      "88/88 [==============================] - 74s 841ms/step - loss: 0.3671 - accuracy: 0.8846\n",
      "Epoch 263/500\n",
      "88/88 [==============================] - 72s 818ms/step - loss: 0.3882 - accuracy: 0.8774\n",
      "Epoch 264/500\n",
      "88/88 [==============================] - 74s 837ms/step - loss: 0.3933 - accuracy: 0.8750\n",
      "Epoch 265/500\n",
      "88/88 [==============================] - 75s 846ms/step - loss: 0.3887 - accuracy: 0.8767\n",
      "Epoch 266/500\n",
      "88/88 [==============================] - 73s 825ms/step - loss: 0.4033 - accuracy: 0.8726\n",
      "Epoch 267/500\n",
      "88/88 [==============================] - 75s 847ms/step - loss: 0.4275 - accuracy: 0.8655\n",
      "Epoch 268/500\n",
      "88/88 [==============================] - 74s 835ms/step - loss: 0.4213 - accuracy: 0.8674\n",
      "Epoch 269/500\n",
      "88/88 [==============================] - 81s 925ms/step - loss: 0.4203 - accuracy: 0.8673\n",
      "Epoch 270/500\n",
      "88/88 [==============================] - 76s 867ms/step - loss: 0.4350 - accuracy: 0.8641\n",
      "Epoch 271/500\n",
      "88/88 [==============================] - 75s 849ms/step - loss: 0.4526 - accuracy: 0.8573\n",
      "Epoch 272/500\n",
      "88/88 [==============================] - 75s 857ms/step - loss: 0.4607 - accuracy: 0.8548\n",
      "Epoch 273/500\n",
      "88/88 [==============================] - 73s 828ms/step - loss: 0.4805 - accuracy: 0.8496\n",
      "Epoch 274/500\n",
      "88/88 [==============================] - 73s 831ms/step - loss: 0.4944 - accuracy: 0.8452\n",
      "Epoch 275/500\n",
      "88/88 [==============================] - 77s 871ms/step - loss: 0.5071 - accuracy: 0.8413\n",
      "Epoch 276/500\n",
      "88/88 [==============================] - 73s 832ms/step - loss: 0.5131 - accuracy: 0.8390\n",
      "Epoch 277/500\n",
      "88/88 [==============================] - 77s 873ms/step - loss: 0.4871 - accuracy: 0.8468\n",
      "Epoch 278/500\n",
      "88/88 [==============================] - 74s 839ms/step - loss: 0.4664 - accuracy: 0.8525\n",
      "Epoch 279/500\n",
      "88/88 [==============================] - 80s 909ms/step - loss: 0.4718 - accuracy: 0.8503\n",
      "Epoch 280/500\n",
      "88/88 [==============================] - 76s 863ms/step - loss: 0.4683 - accuracy: 0.8519\n",
      "Epoch 281/500\n",
      "88/88 [==============================] - 73s 835ms/step - loss: 0.4934 - accuracy: 0.8445\n",
      "Epoch 282/500\n",
      "88/88 [==============================] - 73s 825ms/step - loss: 0.5007 - accuracy: 0.8418\n",
      "Epoch 283/500\n",
      "88/88 [==============================] - 71s 804ms/step - loss: 0.4924 - accuracy: 0.8441\n",
      "Epoch 284/500\n",
      "88/88 [==============================] - 74s 845ms/step - loss: 0.5130 - accuracy: 0.8384\n",
      "Epoch 285/500\n",
      "88/88 [==============================] - 77s 873ms/step - loss: 0.5761 - accuracy: 0.8224\n",
      "Epoch 286/500\n",
      "88/88 [==============================] - 71s 807ms/step - loss: 0.5905 - accuracy: 0.8169\n",
      "Epoch 287/500\n",
      "88/88 [==============================] - 74s 843ms/step - loss: 0.6680 - accuracy: 0.7976\n",
      "Epoch 288/500\n",
      "88/88 [==============================] - 72s 822ms/step - loss: 0.7302 - accuracy: 0.7829\n",
      "Epoch 289/500\n",
      "88/88 [==============================] - 78s 891ms/step - loss: 1.1300 - accuracy: 0.7128\n",
      "Epoch 290/500\n",
      "88/88 [==============================] - 74s 842ms/step - loss: 1.2156 - accuracy: 0.6864\n",
      "Epoch 291/500\n",
      "88/88 [==============================] - 72s 819ms/step - loss: 1.0378 - accuracy: 0.7105\n",
      "Epoch 292/500\n",
      "88/88 [==============================] - 74s 839ms/step - loss: 0.9384 - accuracy: 0.7277\n",
      "Epoch 293/500\n",
      "88/88 [==============================] - 72s 813ms/step - loss: 0.8644 - accuracy: 0.7429\n",
      "Epoch 294/500\n",
      "88/88 [==============================] - 84s 950ms/step - loss: 0.8096 - accuracy: 0.7551\n",
      "Epoch 295/500\n",
      "88/88 [==============================] - 76s 863ms/step - loss: 0.7850 - accuracy: 0.7614\n",
      "Epoch 296/500\n",
      "88/88 [==============================] - 72s 819ms/step - loss: 0.7628 - accuracy: 0.7673\n",
      "Epoch 297/500\n",
      "88/88 [==============================] - 74s 841ms/step - loss: 0.7253 - accuracy: 0.7758\n",
      "Epoch 298/500\n",
      "88/88 [==============================] - 72s 823ms/step - loss: 0.7237 - accuracy: 0.7769\n",
      "Epoch 299/500\n",
      "88/88 [==============================] - 74s 842ms/step - loss: 0.7358 - accuracy: 0.7735\n",
      "Epoch 300/500\n",
      "88/88 [==============================] - 75s 849ms/step - loss: 0.7372 - accuracy: 0.7753\n",
      "Epoch 301/500\n",
      "88/88 [==============================] - 77s 872ms/step - loss: 0.7394 - accuracy: 0.7749\n",
      "Epoch 302/500\n",
      "88/88 [==============================] - 78s 891ms/step - loss: 0.7310 - accuracy: 0.7759\n",
      "Epoch 303/500\n",
      "88/88 [==============================] - 75s 852ms/step - loss: 0.7571 - accuracy: 0.7722\n",
      "Epoch 304/500\n",
      "88/88 [==============================] - 76s 863ms/step - loss: 0.7592 - accuracy: 0.7685\n",
      "Epoch 305/500\n",
      "88/88 [==============================] - 76s 867ms/step - loss: 0.7122 - accuracy: 0.7798\n",
      "Epoch 306/500\n",
      "88/88 [==============================] - 74s 839ms/step - loss: 0.7089 - accuracy: 0.7805\n",
      "Epoch 307/500\n",
      "88/88 [==============================] - 75s 850ms/step - loss: 0.6983 - accuracy: 0.7840\n",
      "Epoch 308/500\n",
      "88/88 [==============================] - 75s 855ms/step - loss: 0.6878 - accuracy: 0.7862\n",
      "Epoch 309/500\n",
      "88/88 [==============================] - 76s 866ms/step - loss: 0.6844 - accuracy: 0.7862\n",
      "Epoch 310/500\n",
      "88/88 [==============================] - 78s 887ms/step - loss: 0.6772 - accuracy: 0.7895\n",
      "Epoch 311/500\n",
      "88/88 [==============================] - 74s 845ms/step - loss: 0.6720 - accuracy: 0.7911\n",
      "Epoch 312/500\n",
      "88/88 [==============================] - 77s 870ms/step - loss: 0.6509 - accuracy: 0.7965\n",
      "Epoch 313/500\n",
      "88/88 [==============================] - 74s 835ms/step - loss: 0.6638 - accuracy: 0.7931\n",
      "Epoch 314/500\n",
      "88/88 [==============================] - 73s 831ms/step - loss: 0.6899 - accuracy: 0.7864\n",
      "Epoch 315/500\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "88/88 [==============================] - 74s 840ms/step - loss: 0.6914 - accuracy: 0.7861\n",
      "Epoch 316/500\n",
      "88/88 [==============================] - 73s 827ms/step - loss: 0.7188 - accuracy: 0.7825\n",
      "Epoch 317/500\n",
      "88/88 [==============================] - 74s 839ms/step - loss: 0.7381 - accuracy: 0.7790\n",
      "Epoch 318/500\n",
      "88/88 [==============================] - 72s 816ms/step - loss: 0.7221 - accuracy: 0.7802\n",
      "Epoch 319/500\n",
      "88/88 [==============================] - 73s 831ms/step - loss: 0.7112 - accuracy: 0.7808\n",
      "Epoch 320/500\n",
      "88/88 [==============================] - 78s 889ms/step - loss: 0.7378 - accuracy: 0.7738\n",
      "Epoch 321/500\n",
      "88/88 [==============================] - 75s 848ms/step - loss: 0.7436 - accuracy: 0.7715\n",
      "Epoch 322/500\n",
      "88/88 [==============================] - 74s 837ms/step - loss: 0.7540 - accuracy: 0.7677\n",
      "Epoch 323/500\n",
      "88/88 [==============================] - 74s 836ms/step - loss: 0.7424 - accuracy: 0.7723\n",
      "Epoch 324/500\n",
      "88/88 [==============================] - 75s 848ms/step - loss: 0.7337 - accuracy: 0.7740\n",
      "Epoch 325/500\n",
      "88/88 [==============================] - 75s 857ms/step - loss: 0.7269 - accuracy: 0.7748\n",
      "Epoch 326/500\n",
      "88/88 [==============================] - 75s 846ms/step - loss: 0.7012 - accuracy: 0.7821\n",
      "Epoch 327/500\n",
      "88/88 [==============================] - 75s 848ms/step - loss: 0.6875 - accuracy: 0.7864\n",
      "Epoch 328/500\n",
      "88/88 [==============================] - 72s 821ms/step - loss: 0.6815 - accuracy: 0.7878\n",
      "Epoch 329/500\n",
      "88/88 [==============================] - 74s 837ms/step - loss: 0.6753 - accuracy: 0.7893\n",
      "Epoch 330/500\n",
      "88/88 [==============================] - 75s 850ms/step - loss: 0.7058 - accuracy: 0.7809\n",
      "Epoch 331/500\n",
      "88/88 [==============================] - 75s 856ms/step - loss: 0.7197 - accuracy: 0.7774\n",
      "Epoch 332/500\n",
      "88/88 [==============================] - 82s 927ms/step - loss: 0.7316 - accuracy: 0.7757\n",
      "Epoch 333/500\n",
      "88/88 [==============================] - 76s 863ms/step - loss: 0.7455 - accuracy: 0.7727\n",
      "Epoch 334/500\n",
      "88/88 [==============================] - 77s 880ms/step - loss: 0.7696 - accuracy: 0.7659\n",
      "Epoch 335/500\n",
      "88/88 [==============================] - 77s 880ms/step - loss: 0.7618 - accuracy: 0.7656\n",
      "Epoch 336/500\n",
      "88/88 [==============================] - 75s 848ms/step - loss: 0.7545 - accuracy: 0.7689\n",
      "Epoch 337/500\n",
      "88/88 [==============================] - 75s 854ms/step - loss: 0.7208 - accuracy: 0.7762\n",
      "Epoch 338/500\n",
      "88/88 [==============================] - 73s 828ms/step - loss: 0.7025 - accuracy: 0.7815\n",
      "Epoch 339/500\n",
      "88/88 [==============================] - 76s 862ms/step - loss: 0.7087 - accuracy: 0.7796\n",
      "Epoch 340/500\n",
      "88/88 [==============================] - 78s 888ms/step - loss: 0.7018 - accuracy: 0.7816\n",
      "Epoch 341/500\n",
      "88/88 [==============================] - 73s 834ms/step - loss: 0.6899 - accuracy: 0.7846\n",
      "Epoch 342/500\n",
      "88/88 [==============================] - 78s 881ms/step - loss: 0.7003 - accuracy: 0.7823\n",
      "Epoch 343/500\n",
      "88/88 [==============================] - 73s 827ms/step - loss: 0.7704 - accuracy: 0.7678\n",
      "Epoch 344/500\n",
      "88/88 [==============================] - 74s 840ms/step - loss: 0.7397 - accuracy: 0.7722\n",
      "Epoch 345/500\n",
      "88/88 [==============================] - 74s 846ms/step - loss: 0.7279 - accuracy: 0.7740\n",
      "Epoch 346/500\n",
      "88/88 [==============================] - 73s 829ms/step - loss: 0.7202 - accuracy: 0.7765\n",
      "Epoch 347/500\n",
      "88/88 [==============================] - 73s 833ms/step - loss: 0.7288 - accuracy: 0.7740\n",
      "Epoch 348/500\n",
      "88/88 [==============================] - 72s 813ms/step - loss: 0.7469 - accuracy: 0.7696\n",
      "Epoch 349/500\n",
      "88/88 [==============================] - 76s 868ms/step - loss: 0.7749 - accuracy: 0.7632\n",
      "Epoch 350/500\n",
      "88/88 [==============================] - 78s 888ms/step - loss: 0.8080 - accuracy: 0.7554\n",
      "Epoch 351/500\n",
      "88/88 [==============================] - 72s 819ms/step - loss: 0.7976 - accuracy: 0.7568\n",
      "Epoch 352/500\n",
      "88/88 [==============================] - 75s 847ms/step - loss: 0.8097 - accuracy: 0.7540\n",
      "Epoch 353/500\n",
      "88/88 [==============================] - 72s 822ms/step - loss: 0.8345 - accuracy: 0.7488\n",
      "Epoch 354/500\n",
      "88/88 [==============================] - 74s 843ms/step - loss: 0.8672 - accuracy: 0.7404\n",
      "Epoch 355/500\n",
      "88/88 [==============================] - 77s 877ms/step - loss: 0.8784 - accuracy: 0.7358\n",
      "Epoch 356/500\n",
      "88/88 [==============================] - 72s 813ms/step - loss: 0.8787 - accuracy: 0.7366\n",
      "Epoch 357/500\n",
      "88/88 [==============================] - 78s 883ms/step - loss: 0.9014 - accuracy: 0.7338\n",
      "Epoch 358/500\n",
      "88/88 [==============================] - 73s 827ms/step - loss: 0.8591 - accuracy: 0.7425\n",
      "Epoch 359/500\n",
      "88/88 [==============================] - 75s 857ms/step - loss: 0.8548 - accuracy: 0.7426\n",
      "Epoch 360/500\n",
      "88/88 [==============================] - 73s 826ms/step - loss: 0.8481 - accuracy: 0.7446\n",
      "Epoch 361/500\n",
      "88/88 [==============================] - 74s 838ms/step - loss: 0.8569 - accuracy: 0.7435\n",
      "Epoch 362/500\n",
      "88/88 [==============================] - 77s 879ms/step - loss: 0.8387 - accuracy: 0.7466\n",
      "Epoch 363/500\n",
      "88/88 [==============================] - 73s 825ms/step - loss: 0.8259 - accuracy: 0.7498\n",
      "Epoch 364/500\n",
      "88/88 [==============================] - 74s 836ms/step - loss: 0.8290 - accuracy: 0.7497\n",
      "Epoch 365/500\n",
      "88/88 [==============================] - 76s 857ms/step - loss: 0.8366 - accuracy: 0.7472\n",
      "Epoch 366/500\n",
      "88/88 [==============================] - 75s 849ms/step - loss: 0.8362 - accuracy: 0.7465\n",
      "Epoch 367/500\n",
      "88/88 [==============================] - 75s 848ms/step - loss: 0.8172 - accuracy: 0.7510\n",
      "Epoch 368/500\n",
      "88/88 [==============================] - 73s 826ms/step - loss: 0.7946 - accuracy: 0.7566\n",
      "Epoch 369/500\n",
      "88/88 [==============================] - 74s 845ms/step - loss: 0.7883 - accuracy: 0.7577\n",
      "Epoch 370/500\n",
      "88/88 [==============================] - 75s 855ms/step - loss: 0.7805 - accuracy: 0.7593\n",
      "Epoch 371/500\n",
      "88/88 [==============================] - 73s 831ms/step - loss: 0.7689 - accuracy: 0.7630\n",
      "Epoch 372/500\n",
      "88/88 [==============================] - 79s 895ms/step - loss: 0.7694 - accuracy: 0.7622\n",
      "Epoch 373/500\n",
      "88/88 [==============================] - 75s 852ms/step - loss: 0.7799 - accuracy: 0.7597\n",
      "Epoch 374/500\n",
      "88/88 [==============================] - 77s 870ms/step - loss: 0.7872 - accuracy: 0.7580\n",
      "Epoch 375/500\n",
      "88/88 [==============================] - 77s 872ms/step - loss: 0.7853 - accuracy: 0.7578\n",
      "Epoch 376/500\n",
      "88/88 [==============================] - 78s 884ms/step - loss: 0.7817 - accuracy: 0.7594\n",
      "Epoch 377/500\n",
      "88/88 [==============================] - 75s 853ms/step - loss: 0.7822 - accuracy: 0.7592\n",
      "Epoch 378/500\n",
      "88/88 [==============================] - 74s 841ms/step - loss: 0.8292 - accuracy: 0.7510\n",
      "Epoch 379/500\n",
      "88/88 [==============================] - 76s 866ms/step - loss: 0.8356 - accuracy: 0.7478\n",
      "Epoch 380/500\n",
      "88/88 [==============================] - 76s 864ms/step - loss: 0.8489 - accuracy: 0.7447\n",
      "Epoch 381/500\n",
      "88/88 [==============================] - 78s 883ms/step - loss: 0.8352 - accuracy: 0.7473\n",
      "Epoch 382/500\n",
      "88/88 [==============================] - 78s 881ms/step - loss: 0.8465 - accuracy: 0.7462\n",
      "Epoch 383/500\n",
      "88/88 [==============================] - 73s 829ms/step - loss: 0.8438 - accuracy: 0.7461\n",
      "Epoch 384/500\n",
      "88/88 [==============================] - 76s 869ms/step - loss: 0.8519 - accuracy: 0.7437\n",
      "Epoch 385/500\n",
      "88/88 [==============================] - 77s 878ms/step - loss: 0.8928 - accuracy: 0.7339\n",
      "Epoch 386/500\n",
      "88/88 [==============================] - 73s 826ms/step - loss: 0.9585 - accuracy: 0.7183\n",
      "Epoch 387/500\n",
      "88/88 [==============================] - 74s 838ms/step - loss: 0.9539 - accuracy: 0.7177\n",
      "Epoch 388/500\n",
      "88/88 [==============================] - 76s 868ms/step - loss: 0.9575 - accuracy: 0.7168\n",
      "Epoch 389/500\n",
      "88/88 [==============================] - 74s 838ms/step - loss: 1.0212 - accuracy: 0.7037\n",
      "Epoch 390/500\n",
      "88/88 [==============================] - 82s 931ms/step - loss: 1.0291 - accuracy: 0.7012\n",
      "Epoch 391/500\n",
      "88/88 [==============================] - 72s 819ms/step - loss: 1.0123 - accuracy: 0.7030\n",
      "Epoch 392/500\n",
      "88/88 [==============================] - 74s 839ms/step - loss: 1.0015 - accuracy: 0.7049\n",
      "Epoch 393/500\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "88/88 [==============================] - 74s 840ms/step - loss: 0.9868 - accuracy: 0.7083\n",
      "Epoch 394/500\n",
      "88/88 [==============================] - 74s 841ms/step - loss: 0.9647 - accuracy: 0.7128\n",
      "Epoch 395/500\n",
      "88/88 [==============================] - 75s 851ms/step - loss: 0.9628 - accuracy: 0.7136\n",
      "Epoch 396/500\n",
      "88/88 [==============================] - 73s 829ms/step - loss: 0.9461 - accuracy: 0.7170\n",
      "Epoch 397/500\n",
      "88/88 [==============================] - 77s 877ms/step - loss: 0.9271 - accuracy: 0.7206\n",
      "Epoch 398/500\n",
      "88/88 [==============================] - 74s 842ms/step - loss: 0.9269 - accuracy: 0.7221\n",
      "Epoch 399/500\n",
      "88/88 [==============================] - 79s 904ms/step - loss: 0.9141 - accuracy: 0.7247\n",
      "Epoch 400/500\n",
      "88/88 [==============================] - 77s 880ms/step - loss: 0.9060 - accuracy: 0.7263\n",
      "Epoch 401/500\n",
      "88/88 [==============================] - 75s 846ms/step - loss: 0.8949 - accuracy: 0.7286\n",
      "Epoch 402/500\n",
      "88/88 [==============================] - 76s 869ms/step - loss: 0.8920 - accuracy: 0.7290\n",
      "Epoch 403/500\n",
      "88/88 [==============================] - 74s 842ms/step - loss: 0.8880 - accuracy: 0.7298\n",
      "Epoch 404/500\n",
      "88/88 [==============================] - 75s 848ms/step - loss: 0.8975 - accuracy: 0.7286\n",
      "Epoch 405/500\n",
      "88/88 [==============================] - 77s 869ms/step - loss: 0.8920 - accuracy: 0.7291\n",
      "Epoch 406/500\n",
      "88/88 [==============================] - 73s 831ms/step - loss: 0.8747 - accuracy: 0.7334\n",
      "Epoch 407/500\n",
      "88/88 [==============================] - 77s 872ms/step - loss: 0.8670 - accuracy: 0.7361\n",
      "Epoch 408/500\n",
      "88/88 [==============================] - 73s 828ms/step - loss: 0.8676 - accuracy: 0.7370\n",
      "Epoch 409/500\n",
      "88/88 [==============================] - 77s 870ms/step - loss: 0.8513 - accuracy: 0.7406\n",
      "Epoch 410/500\n",
      "88/88 [==============================] - 74s 845ms/step - loss: 0.8492 - accuracy: 0.7423\n",
      "Epoch 411/500\n",
      "88/88 [==============================] - 73s 825ms/step - loss: 0.8413 - accuracy: 0.7435\n",
      "Epoch 412/500\n",
      "88/88 [==============================] - 74s 844ms/step - loss: 0.8463 - accuracy: 0.7416\n",
      "Epoch 413/500\n",
      "88/88 [==============================] - 76s 859ms/step - loss: 0.8354 - accuracy: 0.7444\n",
      "Epoch 414/500\n",
      "88/88 [==============================] - 76s 860ms/step - loss: 0.8333 - accuracy: 0.7455\n",
      "Epoch 415/500\n",
      "88/88 [==============================] - 82s 934ms/step - loss: 0.8375 - accuracy: 0.7443\n",
      "Epoch 416/500\n",
      "88/88 [==============================] - 72s 818ms/step - loss: 0.8495 - accuracy: 0.7426\n",
      "Epoch 417/500\n",
      "88/88 [==============================] - 77s 870ms/step - loss: 0.8557 - accuracy: 0.7409\n",
      "Epoch 418/500\n",
      "88/88 [==============================] - 74s 835ms/step - loss: 0.8736 - accuracy: 0.7372\n",
      "Epoch 419/500\n",
      "88/88 [==============================] - 77s 876ms/step - loss: 0.8694 - accuracy: 0.7365\n",
      "Epoch 420/500\n",
      "88/88 [==============================] - 75s 855ms/step - loss: 0.8711 - accuracy: 0.7363\n",
      "Epoch 421/500\n",
      "88/88 [==============================] - 73s 827ms/step - loss: 0.8831 - accuracy: 0.7338\n",
      "Epoch 422/500\n",
      "88/88 [==============================] - 77s 875ms/step - loss: 0.8819 - accuracy: 0.7343\n",
      "Epoch 423/500\n",
      "88/88 [==============================] - 79s 896ms/step - loss: 0.8810 - accuracy: 0.7338\n",
      "Epoch 424/500\n",
      "88/88 [==============================] - 76s 860ms/step - loss: 0.8958 - accuracy: 0.7304\n",
      "Epoch 425/500\n",
      "88/88 [==============================] - 81s 926ms/step - loss: 0.9320 - accuracy: 0.7235\n",
      "Epoch 426/500\n",
      "88/88 [==============================] - 73s 826ms/step - loss: 0.9589 - accuracy: 0.7185\n",
      "Epoch 427/500\n",
      "88/88 [==============================] - 77s 873ms/step - loss: 0.9551 - accuracy: 0.7183\n",
      "Epoch 428/500\n",
      "88/88 [==============================] - 74s 837ms/step - loss: 0.9488 - accuracy: 0.7182\n",
      "Epoch 429/500\n",
      "88/88 [==============================] - 76s 867ms/step - loss: 0.9658 - accuracy: 0.7162\n",
      "Epoch 430/500\n",
      "88/88 [==============================] - 75s 856ms/step - loss: 0.9957 - accuracy: 0.7104\n",
      "Epoch 431/500\n",
      "88/88 [==============================] - 75s 852ms/step - loss: 1.0062 - accuracy: 0.7111\n",
      "Epoch 432/500\n",
      "88/88 [==============================] - 74s 843ms/step - loss: 0.9843 - accuracy: 0.7143\n",
      "Epoch 433/500\n",
      "88/88 [==============================] - 76s 863ms/step - loss: 0.9927 - accuracy: 0.7115\n",
      "Epoch 434/500\n",
      "88/88 [==============================] - 74s 836ms/step - loss: 0.9861 - accuracy: 0.7140\n",
      "Epoch 435/500\n",
      "88/88 [==============================] - 73s 832ms/step - loss: 0.9675 - accuracy: 0.7179\n",
      "Epoch 436/500\n",
      "88/88 [==============================] - 72s 820ms/step - loss: 0.9514 - accuracy: 0.7222\n",
      "Epoch 437/500\n",
      "88/88 [==============================] - 76s 863ms/step - loss: 0.9382 - accuracy: 0.7252\n",
      "Epoch 438/500\n",
      "88/88 [==============================] - 76s 864ms/step - loss: 0.9295 - accuracy: 0.7268\n",
      "Epoch 439/500\n",
      "88/88 [==============================] - 77s 874ms/step - loss: 0.9268 - accuracy: 0.7267\n",
      "Epoch 440/500\n",
      "88/88 [==============================] - 74s 839ms/step - loss: 0.9189 - accuracy: 0.7280\n",
      "Epoch 441/500\n",
      "88/88 [==============================] - 74s 844ms/step - loss: 0.9099 - accuracy: 0.7286\n",
      "Epoch 442/500\n",
      "88/88 [==============================] - 77s 870ms/step - loss: 0.8912 - accuracy: 0.7323\n",
      "Epoch 443/500\n",
      "88/88 [==============================] - 73s 833ms/step - loss: 0.8928 - accuracy: 0.7303\n",
      "Epoch 444/500\n",
      "88/88 [==============================] - 75s 855ms/step - loss: 0.8780 - accuracy: 0.7349\n",
      "Epoch 445/500\n",
      "88/88 [==============================] - 76s 862ms/step - loss: 0.8704 - accuracy: 0.7358\n",
      "Epoch 446/500\n",
      "88/88 [==============================] - 78s 886ms/step - loss: 0.8688 - accuracy: 0.7372\n",
      "Epoch 447/500\n",
      "88/88 [==============================] - 76s 863ms/step - loss: 0.8593 - accuracy: 0.7391\n",
      "Epoch 448/500\n",
      "88/88 [==============================] - 74s 841ms/step - loss: 0.8729 - accuracy: 0.7349\n",
      "Epoch 449/500\n",
      "88/88 [==============================] - 79s 894ms/step - loss: 0.8938 - accuracy: 0.7292\n",
      "Epoch 450/500\n",
      "88/88 [==============================] - 75s 851ms/step - loss: 0.9118 - accuracy: 0.7266\n",
      "Epoch 451/500\n",
      "88/88 [==============================] - 75s 846ms/step - loss: 0.9167 - accuracy: 0.7263\n",
      "Epoch 452/500\n",
      "88/88 [==============================] - 77s 879ms/step - loss: 0.9209 - accuracy: 0.7228\n",
      "Epoch 453/500\n",
      "88/88 [==============================] - 73s 833ms/step - loss: 0.9137 - accuracy: 0.7258\n",
      "Epoch 454/500\n",
      "88/88 [==============================] - 77s 871ms/step - loss: 0.9171 - accuracy: 0.7258\n",
      "Epoch 455/500\n",
      "88/88 [==============================] - 74s 836ms/step - loss: 0.9325 - accuracy: 0.7244\n",
      "Epoch 456/500\n",
      "88/88 [==============================] - 73s 825ms/step - loss: 0.9232 - accuracy: 0.7258\n",
      "Epoch 457/500\n",
      "88/88 [==============================] - 75s 847ms/step - loss: 0.9135 - accuracy: 0.7262\n",
      "Epoch 458/500\n",
      "88/88 [==============================] - 71s 810ms/step - loss: 0.9250 - accuracy: 0.7230\n",
      "Epoch 459/500\n",
      "88/88 [==============================] - 77s 875ms/step - loss: 0.9308 - accuracy: 0.7215\n",
      "Epoch 460/500\n",
      "88/88 [==============================] - 75s 851ms/step - loss: 0.9906 - accuracy: 0.7065\n",
      "Epoch 461/500\n",
      "88/88 [==============================] - 77s 877ms/step - loss: 0.9923 - accuracy: 0.7066\n",
      "Epoch 462/500\n",
      "88/88 [==============================] - 114s 1s/step - loss: 0.9783 - accuracy: 0.7109\n",
      "Epoch 463/500\n",
      "88/88 [==============================] - 107s 1s/step - loss: 0.9960 - accuracy: 0.7084\n",
      "Epoch 464/500\n",
      "88/88 [==============================] - 112s 1s/step - loss: 1.0091 - accuracy: 0.7051\n",
      "Epoch 465/500\n",
      "88/88 [==============================] - 104s 1s/step - loss: 1.0095 - accuracy: 0.7059\n",
      "Epoch 466/500\n",
      "88/88 [==============================] - 104s 1s/step - loss: 1.0103 - accuracy: 0.7032\n",
      "Epoch 467/500\n",
      "88/88 [==============================] - 113s 1s/step - loss: 1.0167 - accuracy: 0.7009\n",
      "Epoch 468/500\n",
      "88/88 [==============================] - 114s 1s/step - loss: 1.0118 - accuracy: 0.7027\n",
      "Epoch 469/500\n",
      "88/88 [==============================] - 83s 949ms/step - loss: 1.0032 - accuracy: 0.7045\n",
      "Epoch 470/500\n",
      "88/88 [==============================] - 76s 860ms/step - loss: 0.9890 - accuracy: 0.7075\n",
      "Epoch 471/500\n",
      "88/88 [==============================] - 74s 842ms/step - loss: 0.9706 - accuracy: 0.7106\n",
      "Epoch 472/500\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "88/88 [==============================] - 77s 872ms/step - loss: 0.9639 - accuracy: 0.7139\n",
      "Epoch 473/500\n",
      "88/88 [==============================] - 72s 817ms/step - loss: 0.9609 - accuracy: 0.7142\n",
      "Epoch 474/500\n",
      "88/88 [==============================] - 74s 836ms/step - loss: 0.9482 - accuracy: 0.7174\n",
      "Epoch 475/500\n",
      "88/88 [==============================] - 75s 846ms/step - loss: 0.9378 - accuracy: 0.7192\n",
      "Epoch 476/500\n",
      "88/88 [==============================] - 72s 823ms/step - loss: 0.9250 - accuracy: 0.7228\n",
      "Epoch 477/500\n",
      "88/88 [==============================] - 74s 840ms/step - loss: 0.9360 - accuracy: 0.7203\n",
      "Epoch 478/500\n",
      "88/88 [==============================] - 72s 813ms/step - loss: 0.9319 - accuracy: 0.7210\n",
      "Epoch 479/500\n",
      "88/88 [==============================] - 77s 873ms/step - loss: 0.9398 - accuracy: 0.7198\n",
      "Epoch 480/500\n",
      "88/88 [==============================] - 74s 835ms/step - loss: 0.9677 - accuracy: 0.7118\n",
      "Epoch 481/500\n",
      "88/88 [==============================] - 72s 820ms/step - loss: 0.9666 - accuracy: 0.7137\n",
      "Epoch 482/500\n",
      "88/88 [==============================] - 83s 945ms/step - loss: 0.9879 - accuracy: 0.7103\n",
      "Epoch 483/500\n",
      "88/88 [==============================] - 75s 848ms/step - loss: 1.0049 - accuracy: 0.7074\n",
      "Epoch 484/500\n",
      "88/88 [==============================] - 74s 845ms/step - loss: 0.9854 - accuracy: 0.7103\n",
      "Epoch 485/500\n",
      "88/88 [==============================] - 73s 830ms/step - loss: 0.9768 - accuracy: 0.7122\n",
      "Epoch 486/500\n",
      "88/88 [==============================] - 74s 846ms/step - loss: 0.9647 - accuracy: 0.7148\n",
      "Epoch 487/500\n",
      "88/88 [==============================] - 80s 908ms/step - loss: 0.9484 - accuracy: 0.7175\n",
      "Epoch 488/500\n",
      "88/88 [==============================] - 82s 936ms/step - loss: 0.9476 - accuracy: 0.7186\n",
      "Epoch 489/500\n",
      "88/88 [==============================] - 81s 919ms/step - loss: 0.9406 - accuracy: 0.7193\n",
      "Epoch 490/500\n",
      "88/88 [==============================] - 76s 866ms/step - loss: 0.9392 - accuracy: 0.7200\n",
      "Epoch 491/500\n",
      "88/88 [==============================] - 75s 855ms/step - loss: 0.9580 - accuracy: 0.7154\n",
      "Epoch 492/500\n",
      "88/88 [==============================] - 75s 849ms/step - loss: 0.9505 - accuracy: 0.7176\n",
      "Epoch 493/500\n",
      "88/88 [==============================] - 72s 815ms/step - loss: 0.9676 - accuracy: 0.7168\n",
      "Epoch 494/500\n",
      "88/88 [==============================] - 74s 842ms/step - loss: 0.9725 - accuracy: 0.7144\n",
      "Epoch 495/500\n",
      "88/88 [==============================] - 73s 834ms/step - loss: 0.9595 - accuracy: 0.7167\n",
      "Epoch 496/500\n",
      "88/88 [==============================] - 73s 827ms/step - loss: 0.9550 - accuracy: 0.7173\n",
      "Epoch 497/500\n",
      "88/88 [==============================] - 73s 830ms/step - loss: 0.9458 - accuracy: 0.7187\n",
      "Epoch 498/500\n",
      "88/88 [==============================] - 74s 843ms/step - loss: 0.9836 - accuracy: 0.7109\n",
      "Epoch 499/500\n",
      "88/88 [==============================] - 75s 853ms/step - loss: 0.9732 - accuracy: 0.7116\n",
      "Epoch 500/500\n",
      "88/88 [==============================] - 74s 838ms/step - loss: 0.9788 - accuracy: 0.7095\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'./RNN_checkpoints/ckpt_500'"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Востановим модель из кеша."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "model = RNNgenerator(vocab_size, embedding_dim, rnn_units)\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.build(tf.TensorShape([1, None]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Генерация текста."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "# Number of characters to generate\n",
    "num_generate = 500\n",
    "\n",
    "# Low temperature results in more predictable text.\n",
    "# Higher temperature results in more surprising text.\n",
    "# Experiment to find the best setting.\n",
    "temperature = 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "def generate_text(model, start_string):\n",
    "    # Evaluation step (generating text using the learned model)\n",
    "\n",
    "    # Converting our start string to numbers (vectorizing)\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    # Empty string to store our results\n",
    "    text_generated = []\n",
    "\n",
    "    # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        \n",
    "        # using a categorical distribution to predict the character returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
    "\n",
    "        # Pass the predicted character as the next input to the model\n",
    "        # along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "text_ = generate_text(model, start_string=u\"И вот идет уже \")\n",
    "print(text_)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "И вот идет уже тось          Кобренемитихо),\n",
      "     некахорелы;\n",
      " вышилю.     ныенкродннедестодей на    ут,\n",
      "   увуголера   за\n",
      "     похобе егелыбалиназураго,\n",
      "    Тусьяна  скодит нибназягнодумнеянила               буЯ   мо   X,\n",
      "     жахотинь   итей;\n",
      "    .      лою     жей      Ты,\n",
      "  Я     нахомитла             восег  детумашет     пизьгеты      ся.    не               Песконы Кокана\n",
      "\n",
      "        Iедетининажеротохов Ода,зит.          им\n",
      "               зом?    вара,\n",
      "\n",
      "      Одвыторю Я. т     . .\n",
      "  Мевары.                 \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### На 200-ой эпохе вродебы к-во было не плохим. Посмотрим как пройдет генерация текста. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "model = RNNgenerator(vocab_size, embedding_dim, rnn_units) \n",
    "model.load_weights('./RNN_checkpoints/ckpt_200')\n",
    "model.build(tf.TensorShape([1, None]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "text_ = generate_text(model, start_string=u\"И вот идет уже \")\n",
    "print(text_)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "И вот идет уже сяе  нан;\n",
      "        ужей        бле,                   Году,      имней         жных                     Ко              .\n",
      "   поленаблою           уй\n",
      "       Вышижечеволяхому    ух стой.\n",
      "        ги    бнама;\n",
      "   ОтлилаблевТа,\n",
      " пой\n",
      "   мгитыево     ди      поенниеслам  M}  ромой;   полноташидемером           Сочево     Ножедисевестря         му         Дойст     Вова  ноимихою   Мово,      Охией        млинегоченилы.\n",
      "                  Вый     Гло Кадвыт         скла      бытесатей,\n",
      "    Три            \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Дальше можно не смотреть.\n",
    "\n",
    "Хотел проверить, как поведет себя модель с установленным параметром \"return_state=True\".(подсмотрел в методичке по [tensorflow]( https://www.tensorflow.org/text/tutorials/text_generation)), но видно придется отложить эти эксперименты на потом. Т.к. обучается модель ну очень долго!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Попробуем обучить модель с сохранением состояния."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "class RNNgenerator_1(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "    super().__init__(self)\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru1 = tf.keras.layers.GRU(rnn_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    self.gru2 = tf.keras.layers.GRU(rnn_units,\n",
    "                                    return_sequences=True,\n",
    "                                    return_state=True,\n",
    "                                    recurrent_initializer='glorot_uniform')\n",
    "    self.gru3 = tf.keras.layers.GRU(rnn_units,\n",
    "                                    return_sequences=True,\n",
    "                                    return_state=True,\n",
    "                                    recurrent_initializer='glorot_uniform')\n",
    "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "  def call(self, x, states=None, return_state=False):\n",
    "\n",
    "    x = self.embedding(x)\n",
    "\n",
    "    if states is None:\n",
    "      states = self.gru1.get_initial_state(x)\n",
    "\n",
    "    x, states = self.gru1(x, initial_state=states)\n",
    "    x, states = self.gru2(x, initial_state=states)\n",
    "    x, states = self.gru3(x, initial_state=states)\n",
    "\n",
    "    x = self.dense(x)\n",
    "\n",
    "    if return_state:\n",
    "      return x, states\n",
    "    else:\n",
    "      return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './RNN_1_checkpoints'\n",
    "\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "                                filepath=checkpoint_prefix,\n",
    "                                save_freq=1,\n",
    "                                save_weights_only=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "model_1 = RNNgenerator_1(\n",
    "                 vocab_size= vocab_size,\n",
    "                 embedding_dim=embedding_dim,\n",
    "                 rnn_units=rnn_units,\n",
    "                )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "model_1.compile(optimizer='adam', loss=loss, metrics='accuracy')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "EPOCHS = 200"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "history = model_1.fit(dataset,\n",
    "                    epochs=EPOCHS,\n",
    "                    callbacks=[checkpoint_callback])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/200\n",
      "19/88 [=====>........................] - ETA: 12:08 - loss: 2.8946 - accuracy: 0.5307"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6254/1853230506.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m history = model_1.fit(dataset,\n\u001b[0m\u001b[1;32m      2\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                     callbacks=[checkpoint_callback])\n",
      "\u001b[0;32m~/anaconda3/envs/imageai_env/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 _r=1):\n\u001b[1;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/imageai_env/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/imageai_env/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/imageai_env/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3021\u001b[0m       (graph_function,\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3023\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/imageai_env/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1960\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/anaconda3/envs/imageai_env/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/imageai_env/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Генерация текста."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def generate_text_1(model, start_string, states, temperature):\n",
    "    # Evaluation step (generating text using the learned model)\n",
    "\n",
    "    # Converting our start string to numbers (vectorizing)\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    # Empty string to store our results\n",
    "    text_generated = []\n",
    "\n",
    "    # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions, states = model(input_eval, states=states, return_state=True)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        \n",
    "        # using a categorical distribution to predict the character returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
    "\n",
    "        # Pass the predicted character as the next input to the model\n",
    "        # along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "num_generate = 500\n",
    "temperature = 0.7\n",
    "states = None\n",
    "next_char = u\"И вот идет уже \"\n",
    "text_generated=[]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "text = generate_text_1(model_1, next_char, states, temperature)\n",
    "print(text)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model 2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class RNNgenerator_2(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "    super().__init__(self)\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru1 = tf.keras.layers.GRU(rnn_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    self.gru2 = tf.keras.layers.GRU(rnn_units,\n",
    "                                    return_sequences=True,\n",
    "                                    return_state=True,\n",
    "                                    recurrent_initializer='glorot_uniform')\n",
    "    self.gru3 = tf.keras.layers.GRU(rnn_units,\n",
    "                                    return_sequences=True,\n",
    "                                    return_state=True,\n",
    "                                    recurrent_initializer='glorot_uniform')\n",
    "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "  def call(self, x, states_1=None, states_2=None, states_3=None, return_state=False):\n",
    "\n",
    "    x = self.embedding(x)\n",
    "\n",
    "    if states is None:\n",
    "      states_1 = self.gru1.get_initial_state(x)\n",
    "      states_2 = self.gru1.get_initial_state(x)\n",
    "      states_3 = self.gru1.get_initial_state(x)\n",
    "\n",
    "    x, states_1 = self.gru1(x, initial_state=states_1)\n",
    "    x, states_2 = self.gru2(x, initial_state=states_2)\n",
    "    x, states_3 = self.gru3(x, initial_state=states_3)\n",
    "\n",
    "    x = self.dense(x)\n",
    "\n",
    "    if return_state:\n",
    "      return x, states_1, states_2, states_3\n",
    "    else:\n",
    "      return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './RNN_2_checkpoints'\n",
    "\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "                                filepath=checkpoint_prefix,\n",
    "                                save_freq=1,\n",
    "                                save_weights_only=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_2 = RNNgenerator_2(\n",
    "                 vocab_size= vocab_size,\n",
    "                 embedding_dim=embedding_dim,\n",
    "                 rnn_units=rnn_units,\n",
    "                )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_2.compile(optimizer='adam', loss=loss, metrics='accuracy')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "history = model_2.fit(dataset,\n",
    "                    epochs=EPOCHS,\n",
    "                    callbacks=[checkpoint_callback])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def generate_text_2(model, start_string, states_1, states_2, states_3, temperature):\n",
    "    # Evaluation step (generating text using the learned model)\n",
    "\n",
    "    # Converting our start string to numbers (vectorizing)\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    # Empty string to store our results\n",
    "    text_generated = []\n",
    "\n",
    "    # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions, states_1,states_2, states_3 = model(input_eval, states_1=states_1, states_2=states_2, states_3=states_3, return_state=True)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        \n",
    "        # using a categorical distribution to predict the character returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
    "\n",
    "        # Pass the predicted character as the next input to the model\n",
    "        # along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "num_generate = 500\n",
    "temperature = 0.7\n",
    "states = None\n",
    "next_char = u\"И вот идет уже \"\n",
    "text_generated=[]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "text = generate_text_2(model_2, next_char, states_1, states_2, states_3, temperature)\n",
    "print(text)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6b7c3ebca6b5b4a4b8af7b3624595a453b0710866dad47574474daaa4a66fc12"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('imageai_env': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}